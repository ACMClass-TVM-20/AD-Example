{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Before:\n","@tvm.script.ir_module\n","class Module:\n","    @R.function\n","    def main(x: Tensor((1, 784), \"float32\"), w0: Tensor((784, 128), \"float32\"), b0: Tensor((128,), \"float32\"), w1: Tensor((128, 10), \"float32\"), b1: Tensor((10,), \"float32\"), label: Tensor((1, 10), \"float32\")) -> Tuple(Tensor(None, \"float32\", ndim = 2), Tensor(None, \"float32\", ndim = 0)):\n","        # block 0\n","        with R.dataflow():\n","            lv0: Tensor((1, 784), \"float32\") = relax.matmul(x, w0)\n","            lv1: Tensor(R.call_packed(\"vm.binary_broadcast_shape_infer\", lv0, b0), \"float32\") = relax.add(lv0, b0)\n","            lv2: Tensor(None, \"float32\", ndim = 2) = relax.nn.relu(lv1)\n","            lv3: Tensor(None, \"float32\", ndim = 2) = relax.matmul(lv2, w1)\n","            out: Tensor(None, \"float32\", ndim = 2) = relax.add(lv3, b1)\n","            loss: Tensor(None, \"float32\", ndim = 0) = relax.nn.softmax_cross_entropy(out, label)\n","            R.output(out, loss)\n","        return (out, loss)\n","\n","\n","# After:\n","@tvm.script.ir_module\n","class Module:\n","    @R.function\n","    def main(x: Tensor((1, 784), \"float32\"), w0: Tensor((784, 128), \"float32\"), b0: Tensor((128,), \"float32\"), w1: Tensor((128, 10), \"float32\"), b1: Tensor((10,), \"float32\"), label: Tensor((1, 10), \"float32\")) -> Tuple(Tuple(Tensor(None, \"float32\", ndim = 2), Tensor(None, \"float32\", ndim = 0)), Tensor(None, \"float32\", ndim = 2), Tensor(None, \"float32\", ndim = 1), Tensor(None, \"float32\", ndim = 2), Tensor(None, \"float32\", ndim = 1)):\n","        # block 0\n","        with R.dataflow():\n","            lv0: Tensor((1, 784), \"float32\") = relax.matmul(x, w0)\n","            lv1: Tensor(R.call_packed(\"vm.binary_broadcast_shape_infer\", lv0, b0), \"float32\") = relax.add(lv0, b0)\n","            lv2: Tensor(None, \"float32\", ndim = 2) = relax.nn.relu(lv1)\n","            lv3: Tensor(None, \"float32\", ndim = 2) = relax.matmul(lv2, w1)\n","            out: Tensor(None, \"float32\", ndim = 2) = relax.add(lv3, b1)\n","            loss: Tensor(None, \"float32\", ndim = 0) = relax.nn.softmax_cross_entropy(out, label)\n","            loss_adjoint: Tensor(relax.shape_of(loss), \"float32\") = relax.ones_like(loss)\n","            lv: Tensor(None, \"float32\", ndim = 2) = relax.nn.softmax(out)\n","            out_adjoint: Tensor(relax.shape_of(out), \"float32\") = relax.sub(lv, label)\n","            lv3_adjoint: Tensor(relax.shape_of(lv3), \"float32\") = relax.collapse_sum_like(out_adjoint, lv3)\n","            lv11: Tensor((10, 128), \"float32\") = relax.transpose(w1)\n","            lv21: Tensor(None, \"float32\", ndim = 2) = relax.matmul(lv3_adjoint, lv11)\n","            lv2_adjoint: Tensor(relax.shape_of(lv2), \"float32\") = relax.collapse_sum_like(lv21, lv2)\n","            lv31: Tensor(None, \"float32\", ndim = 2) = relax.nn.gradrelu_(lv1)\n","            lv1_adjoint: Tensor(R.call_packed(\"vm.binary_broadcast_shape_infer\", lv0, b0), \"float32\") = relax.multiply(lv2_adjoint, lv31)\n","            lv0_adjoint: Tensor((1, 784), \"float32\") = relax.collapse_sum_like(lv1_adjoint, lv0)\n","            lv4: Tensor((784, 1), \"float32\") = relax.transpose(x)\n","            lv5: Tensor((784, 1), \"float32\") = relax.matmul(lv4, lv0_adjoint)\n","            w0_adjoint: Tensor((784, 128), \"float32\") = relax.collapse_sum_like(lv5, w0)\n","            b0_adjoint: Tensor((128,), \"float32\") = relax.collapse_sum_like(lv1_adjoint, b0)\n","            lv6: Tensor(None, \"float32\", ndim = 2) = relax.transpose(lv2)\n","            lv7: Tensor(None, \"float32\", ndim = 2) = relax.matmul(lv6, lv3_adjoint)\n","            w1_adjoint: Tensor((128, 10), \"float32\") = relax.collapse_sum_like(lv7, w1)\n","            b1_adjoint: Tensor((10,), \"float32\") = relax.collapse_sum_like(out_adjoint, b1)\n","            R.output(out, loss, w0_adjoint, b0_adjoint, w1_adjoint, b1_adjoint)\n","        return ((out, loss), w0_adjoint, b0_adjoint, w1_adjoint, b1_adjoint)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":2}
