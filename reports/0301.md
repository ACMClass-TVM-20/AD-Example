```python
@R.function
def backbone(input_instances, trainable_params, input_of_states):
    # input_instances, trainable_params and input_of_states denote a number of
    # parameters. input_of_states, output_of_states can be empty.
    #
    # You can update the model states, such as `running_mean` and `running_var` in
    # `batch_norm`, in input_of_states and output_of_states
    return prediction_outputs, output_of_states

@R.function
def loss(backbone_prediction_outputs, targets):
    # backbone_prediction_outputs and targets denote a number of parameters
    # loss should be a scalar Var
    return loss

--->
# placed under relax.transform`
AppendLoss("backbone", loss, num_backbone_outputs=3, new_func_name="backbone_loss")(mod)

@R.function
def backbone_loss(input_instances, trainable_params, input_of_states, targets):
    # backbone_loss contains all bindings in backbone and loss
    return loss, output_of_states

Gradient("backbone_loss", require_grads=mod["backbone_loss"].params[2:4], target_index=0)
```
