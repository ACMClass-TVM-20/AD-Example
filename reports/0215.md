# Works in 0201-0215

### Refactored Optimizer and Gradient

### Finished Draft for AD

### Registered Several Ops
- conv2d_transpose
- repeat
- tile
- Gradient Ops for conv2d, max_pooling(to be removed)

### Registered gradients for Ops
- nll_loss
- conv2d (now self-contained)

```python
@register_gradient("relax.nn.conv2d")
def conv2d_grad(
    orig_var: Var,
    orig_call: Call,
    output_grad: Var,
    ctx: BlockBuilder,
) -> List[Expr]:
    attrs = orig_call.attrs
    assert attrs.data_layout == "NCHW", "only support NCHW data layout"
    assert attrs.kernel_layout == "OIHW", "only support OIHW kernel layout"
    assert attrs.out_layout == "NCHW", "only support NCHW output layout"

    assert len(attrs.padding) == 4
    assert len(attrs.strides) == 2
    assert len(attrs.dilation) == 2

    # calculate output_padding
    data, weight = orig_call.args
    batch, out_channel, grad_h, grad_w = _get_shape(orig_var)
    _, in_channel, in_h, in_w = _get_shape(data)
    _, _, filter_h, filter_w = _get_shape(weight)

    fpad_top, fpad_left, fpad_bottom, fpad_right = attrs.padding
    stride_h, stride_w = attrs.strides
    dilation_h, dilation_w = attrs.dilation

    out_h = (grad_h - 1) * stride_h - fpad_top - fpad_bottom + filter_h
    out_w = (grad_w - 1) * stride_w - fpad_left - fpad_right + filter_w

    output_padding = (in_h - out_h, in_w - out_w)

    data_grad = conv2d_transpose(  # type: ignore
        output_grad,
        orig_call.args[1],
        attrs.strides,
        attrs.padding,
        output_padding,
        attrs.dilation,
        attrs.groups,
        attrs.out_layout,
        attrs.kernel_layout[1] + attrs.kernel_layout[0] + attrs.kernel_layout[2:],
        attrs.data_layout,
        attrs.out_dtype,
    )

    grad = ctx.normalize(tile(output_grad, [1, in_channel // attrs.groups, 1, 1]))
    grad = ctx.normalize(reshape(grad, [-1, 1, 0, 0]))  # batch * oc * ic // groups, 1, oh, ow
    data = ctx.normalize(reshape(data, [1, -1, 0, 0]))  # 1, batch * ic, ih, iw

    weight_grad = ctx.normalize(
        conv2d(
            data,
            grad,
            strides=attrs.dilation,
            padding=attrs.padding,
            dilation=attrs.strides,
            groups=int(in_channel * batch),
            out_dtype=attrs.out_dtype,
        )
    )

    # infer shape of backward_weight
    padded_weight_grad_h = (
        in_h - (grad_h - 1) * stride_h - 1 + fpad_top + fpad_bottom
    ) // dilation_h + 1
    padded_weight_grad_w = (
        in_w - (grad_w - 1) * stride_w - 1 + fpad_left + fpad_right
    ) // dilation_w + 1

    weight_grad = ctx.normalize(
        reshape(
            weight_grad,
            [
                batch,
                in_channel // attrs.groups,
                out_channel,
                padded_weight_grad_h,
                padded_weight_grad_w,
            ],
        )
    )
    weight_grad = ctx.normalize(sum(weight_grad, axis=0))
    weight_grad = ctx.normalize(permute_dims(weight_grad, [1, 0, 2, 3]))

    assert padded_weight_grad_h >= filter_h
    assert padded_weight_grad_w >= filter_w

    if padded_weight_grad_h > filter_h or padded_weight_grad_w > filter_w:
        weight_grad = ctx.normalize(
            strided_slice(
                weight_grad,
                axes=[0, 1, 2, 3],
                begin=[0, 0, 0, 0],
                end=[out_channel, in_channel // attrs.groups, filter_h, filter_w],
            )
        )

    return [data_grad, weight_grad]

```
- max_pooling

### Fixed bugs
- Ops
    - Support weight in nll_loss
    - strides for strided_slice
    - 0 parameter for reshape
- Topi
    - topi.nll_loss
    - topi.nn.pooling
- Language
    - R.ShapeExpr
    - Fix int32 for tir
### 3 PRs are on the road
