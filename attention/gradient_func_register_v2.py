from typing import List, Union
from tvm import relax, tir, te, topi
from tvm.script.parser import relax as R, tir as T, ir as I
from tvm.relax.testing import nn


# RFC: Register operator forward and backward functions

# Note:
# 1. forward vs forward_save_state
#    The forward function only returns the output, while forward_save_state returns the output and the saved tensors.
#    So the forward function may save some computation.
#    In gradient impl, there are three stages: forward ... -> checkpointing ... -> backward
#    - In forward stage, we call the forward function;
#    - in checkpointing stage, we call the forward_save_state function;
#    - in backward stage, we call the backward function.
#
#    This need to be discussed. We can also only use the forward_save_state function. That means
#    giving up the optimization, but the interface is simpler.


# 1. tir/te functions (defined by users)
def attention_func():
    pass


def attention_func_simple():
    pass


def attention_backward_func():
    pass


# 2. module with operators (generated by relax.testing.autograd.Function.apply)
@I.ir_module
class Module:
    @R.function
    def forward_save_state(q: R.Tensor, k: R.Tensor, v: R.Tensor, scale: relax.Constant):
        # If return a 2-tuple, the first is the output, the second is the saved tensors
        # If return one element, it is the output
        res = R.call_tir(attention_func, (q, k, v, scale))
        L = res[1]
        O = res[0]
        return O, (q, k, v, L, scale)

    @R.function
    def forward(q: R.Tensor, k: R.Tensor, v: R.Tensor, scale: relax.Constant):
        O = R.call_tir(attention_func_simple, q, k, v, scale)
        return O

    @R.function
    def backward(dO: R.Tensor, q, k, v, L, scale) -> Union[List[R.Tensor], R.Tensor]:
        res = R.call_tir(attention_backward_func, (dO, q, k, v, L, scale))
        dQ = res[0]
        dK = res[1]
        dV = res[2]
        return dQ, dK, dV

    @R.function
    def main(q: R.Tensor, k: R.Tensor, v: R.Tensor, scale: relax.Constant):
        cls = Module
        O = R.call_op_with_grad(cls.forward, (q, k, v, scale), (cls.forward_save_state, cls.backward), require_grad=[0, 1, 2])
        return O


# 3. user interface (defined by tvm in relax.testing.autograd)

_save_list: List[List[relax.Var]] = []
_require_grad_index: List[List[int]] = []


def save_for_backward(*args):
    """Save states for backward computation."""
    assert len(_save_list) != 0
    _save_list[-1].append(args)


def set_input_grad(indices: List[int]):
    """Set the indices of input tensors that require gradient. If it is not called, the default
    is all inputs require gradient."""
    _require_grad_index[-1].extend(indices)


class Function:
    @staticmethod
    def forward_save_state(Q: R.Tensor, K: R.Tensor, V: R.Tensor, scale):
        # defined by users
        res = nn.emit_te(attention_func, Q, K, V, scale)
        O = nn.emit(res[0])
        L = nn.emit(res[1])
        save_for_backward(Q, K, V, L, scale)
        set_input_grad([0, 1, 2])
        return O

    @staticmethod
    def forward(Q: R.Tensor, K: R.Tensor, V: R.Tensor, scale):
        # defined by users
        O = nn.emit_te(attention_func_simple, Q, K, V, scale)
        return O

    @staticmethod
    def backward(dO, Q: R.Tensor, K: R.Tensor, V: R.Tensor, L, scale):
        # defined by users
        res = nn.emit_te(attention_backward_func, dO, Q, K, V, L, scale)
        return res

    @classmethod
    def apply(cls, *args):
        bb: relax.BlockBuilder = relax.BlockBuilder.current()
        forward_args = [relax.Var(arg.name_hint, arg.struct_info) for arg in args]
        with bb.function("forward_save_state", forward_args):
            with bb.dataflow():
                _save_list.append([])
                _require_grad_index.append([])

                res = cls.forward_save_state(*forward_args)

                out = bb.emit_output(res)

                saved_tensors = [bb.emit_output(arg) for arg in _save_list[-1]]
                require_grad = _require_grad_index[-1]
                _save_list.pop()
                _require_grad_index.pop()

            gvar_forward_with_state = bb.emit_func_output((out, saved_tensors))

        forward_only_args = [relax.Var(arg.name_hint, arg.struct_info) for arg in args]
        with bb.function("forward", forward_only_args):
            with bb.dataflow():
                res = cls.forward_save_state(*forward_only_args)
                out = bb.emit_output(res)
            gvar_forward = bb.emit_func_output(out)

        backward_args = [relax.Var(arg.name_hint, arg.struct_info) for arg in [out] + saved_tensors]
        with bb.function("backward", backward_args):
            with bb.dataflow():
                res = cls.backward(*backward_args)
                out = bb.emit_output(res)
            gvar_backward = bb.emit_func_output(out)

        out = nn.emit(
            relax.call_op_with_grad(
                gvar_forward,
                args,
                (gvar_forward_with_state, gvar_backward),
                require_grad=require_grad,
            )
        )
        return out

# 4. usage example:
class FlashAttention(Function):
    @staticmethod
    def forward_save_state(Q: R.Tensor, K: R.Tensor, V: R.Tensor, scale):
        res = nn.emit_te(attention_func, Q, K, V, scale)
        O = nn.emit(res[0])
        L = nn.emit(res[1])
        save_for_backward(Q, K, V, L, scale)
        set_input_grad([0, 1, 2])
        return O

    @staticmethod
    def forward(Q: R.Tensor, K: R.Tensor, V: R.Tensor, scale):
        O = nn.emit_te(attention_func_simple, Q, K, V, scale)
        return O

    @staticmethod
    def backward(dO, Q: R.Tensor, K: R.Tensor, V: R.Tensor, L, scale):
        res = nn.emit_te(attention_backward_func, dO, Q, K, V, L, scale)
        return res


flash_attn = FlashAttention.apply
