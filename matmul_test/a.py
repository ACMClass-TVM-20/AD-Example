@I.ir_module
class Module:
    @T.prim_func
    def default_function(B: T.Buffer((T.int64(4096), T.int64(4096)), "float16"), A: T.Buffer((T.int64(1), T.int64(4096), T.int64(4096)), "float16"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(4096), T.int64(4096)), "float16")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1024)
        threadIdx_z = T.launch_thread("threadIdx.z", T.int64(2))
        threadIdx_y = T.launch_thread("threadIdx.y", T.int64(2))
        threadIdx_x = T.launch_thread("threadIdx.x", T.int64(32))
        matmul_reindex_shared_dyn = T.allocate([16384], "float32", "shared.dyn")
        matmul_reindex_shared_dyn_warp = T.allocate([4096], "float32", "warp")
        for ax1_0_3_init, ax2_0_3_init in T.grid(4, 4):
            T.mma_fill("float32", 8, matmul_reindex_shared_dyn_warp, ax1_0_3_init * 1024 + ax2_0_3_init * 256)
        with T.allocate([16384], "float16", "shared.dyn") as A_reindex_shared_dyn:
            T_transpose_reindex_shared_dyn = T.allocate([16384], "float16", "shared.dyn")
            A_reindex_shared_dyn_1 = T.Buffer((T.int64(16384),), "float16", data=A_reindex_shared_dyn, scope="shared.dyn")
            A_1 = T.Buffer((T.int64(16777216),), "float16", data=A.data)
            var_T_transpose_intermediate_reindex_shared_dyn = T.Buffer((T.int64(16384),), "float16", data=T_transpose_reindex_shared_dyn, scope="shared.dyn")
            B_1 = T.Buffer((T.int64(16777216),), "float16", data=B.data)
            for ax3_0_0 in T.unroll(T.int64(3)):
                T.attr(0, "async_commit_queue_scope", 0)
                with T.attr(0, "async_scope", 1):
                    for ax0_ax1_fused_0 in range(T.int64(4)):
                        A_reindex_shared_dyn_1[ax3_0_0 * T.int64(4096) + ax0_ax1_fused_0 * T.int64(1024) + threadIdx_z * T.int64(512) + threadIdx_y * T.int64(256) + threadIdx_x // T.int64(4) * T.int64(32) + T.bitwise_xor(threadIdx_x % T.int64(4), threadIdx_x // T.int64(8)) * T.int64(8):ax3_0_0 * T.int64(4096) + ax0_ax1_fused_0 * T.int64(1024) + threadIdx_z * T.int64(512) + threadIdx_y * T.int64(256) + threadIdx_x // T.int64(4) * T.int64(32) + T.bitwise_xor(threadIdx_x % T.int64(4), threadIdx_x // T.int64(8)) * T.int64(8) + T.int64(8)] = A_1[T.Cast("int64", blockIdx_x) // T.int64(32) * T.int64(524288) + ax0_ax1_fused_0 * T.int64(131072) + threadIdx_z * T.int64(65536) + threadIdx_y * T.int64(32768) + threadIdx_x // T.int64(4) * T.int64(4096) + ax3_0_0 * T.int64(32) + threadIdx_x % T.int64(4) * T.int64(8):T.Cast("int64", blockIdx_x) // T.int64(32) * T.int64(524288) + ax0_ax1_fused_0 * T.int64(131072) + threadIdx_z * T.int64(65536) + threadIdx_y * T.int64(32768) + threadIdx_x // T.int64(4) * T.int64(4096) + ax3_0_0 * T.int64(32) + threadIdx_x % T.int64(4) * T.int64(8) + T.int64(8)]
                T.attr(0, "async_scope", 1)
                for ax0_ax1_fused_0 in range(T.int64(4)):
                    var_T_transpose_intermediate_reindex_shared_dyn[ax3_0_0 * T.int64(4096) + ax0_ax1_fused_0 * T.int64(1024) + threadIdx_z * T.int64(512) + threadIdx_y * T.int64(256) + threadIdx_x // T.int64(4) * T.int64(32) + T.bitwise_xor(threadIdx_x % T.int64(4), threadIdx_x // T.int64(8)) * T.int64(8):ax3_0_0 * T.int64(4096) + ax0_ax1_fused_0 * T.int64(1024) + threadIdx_z * T.int64(512) + threadIdx_y * T.int64(256) + threadIdx_x // T.int64(4) * T.int64(32) + T.bitwise_xor(threadIdx_x % T.int64(4), threadIdx_x // T.int64(8)) * T.int64(8) + T.int64(8)] = B_1[T.Cast("int64", blockIdx_x) % T.int64(32) * T.int64(524288) + ax0_ax1_fused_0 * T.int64(131072) + threadIdx_z * T.int64(65536) + threadIdx_y * T.int64(32768) + threadIdx_x // T.int64(4) * T.int64(4096) + ax3_0_0 * T.int64(32) + threadIdx_x % T.int64(4) * T.int64(8):T.Cast("int64", blockIdx_x) % T.int64(32) * T.int64(524288) + ax0_ax1_fused_0 * T.int64(131072) + threadIdx_z * T.int64(65536) + threadIdx_y * T.int64(32768) + threadIdx_x // T.int64(4) * T.int64(4096) + ax3_0_0 * T.int64(32) + threadIdx_x % T.int64(4) * T.int64(8) + T.int64(8)]
            for ax3_0_0 in range(T.int64(125)):
                with T.attr(0, "async_commit_queue_scope", 0):
                    with T.attr(0, "async_scope", 1):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            A_reindex_shared_dyn_1[(ax3_0_0 + T.int64(3)) % T.int64(4) * T.int64(4096) + ax0_ax1_fused_0 * T.int64(1024) + threadIdx_z * T.int64(512) + threadIdx_y * T.int64(256) + threadIdx_x // T.int64(4) * T.int64(32) + T.bitwise_xor(threadIdx_x % T.int64(4), threadIdx_x // T.int64(8)) * T.int64(8):(ax3_0_0 + T.int64(3)) % T.int64(4) * T.int64(4096) + ax0_ax1_fused_0 * T.int64(1024) + threadIdx_z * T.int64(512) + threadIdx_y * T.int64(256) + threadIdx_x // T.int64(4) * T.int64(32) + T.bitwise_xor(threadIdx_x % T.int64(4), threadIdx_x // T.int64(8)) * T.int64(8) + T.int64(8)] = A_1[T.Cast("int64", blockIdx_x) // T.int64(32) * T.int64(524288) + ax0_ax1_fused_0 * T.int64(131072) + threadIdx_z * T.int64(65536) + threadIdx_y * T.int64(32768) + threadIdx_x // T.int64(4) * T.int64(4096) + ax3_0_0 * T.int64(32) + threadIdx_x % T.int64(4) * T.int64(8) + T.int64(96):T.Cast("int64", blockIdx_x) // T.int64(32) * T.int64(524288) + ax0_ax1_fused_0 * T.int64(131072) + threadIdx_z * T.int64(65536) + threadIdx_y * T.int64(32768) + threadIdx_x // T.int64(4) * T.int64(4096) + ax3_0_0 * T.int64(32) + threadIdx_x % T.int64(4) * T.int64(8) + T.int64(96) + T.int64(8)]
                    T.attr(0, "async_scope", 1)
                    for ax0_ax1_fused_0 in range(T.int64(4)):
                        var_T_transpose_intermediate_reindex_shared_dyn[(ax3_0_0 + T.int64(3)) % T.int64(4) * T.int64(4096) + ax0_ax1_fused_0 * T.int64(1024) + threadIdx_z * T.int64(512) + threadIdx_y * T.int64(256) + threadIdx_x // T.int64(4) * T.int64(32) + T.bitwise_xor(threadIdx_x % T.int64(4), threadIdx_x // T.int64(8)) * T.int64(8):(ax3_0_0 + T.int64(3)) % T.int64(4) * T.int64(4096) + ax0_ax1_fused_0 * T.int64(1024) + threadIdx_z * T.int64(512) + threadIdx_y * T.int64(256) + threadIdx_x // T.int64(4) * T.int64(32) + T.bitwise_xor(threadIdx_x % T.int64(4), threadIdx_x // T.int64(8)) * T.int64(8) + T.int64(8)] = B_1[T.Cast("int64", blockIdx_x) % T.int64(32) * T.int64(524288) + ax0_ax1_fused_0 * T.int64(131072) + threadIdx_z * T.int64(65536) + threadIdx_y * T.int64(32768) + threadIdx_x // T.int64(4) * T.int64(4096) + ax3_0_0 * T.int64(32) + threadIdx_x % T.int64(4) * T.int64(8) + T.int64(96):T.Cast("int64", blockIdx_x) % T.int64(32) * T.int64(524288) + ax0_ax1_fused_0 * T.int64(131072) + threadIdx_z * T.int64(65536) + threadIdx_y * T.int64(32768) + threadIdx_x // T.int64(4) * T.int64(4096) + ax3_0_0 * T.int64(32) + threadIdx_x % T.int64(4) * T.int64(8) + T.int64(96) + T.int64(8)]
                T.attr(0, "async_wait_queue_scope", 0)
                T.attr(0, "async_wait_inflight_count", T.int64(3))
                for ax3_0_1 in range(T.int64(2)):
                    A_reindex_shared_dyn_warp = T.allocate([1024], "float16", "warp")
                    T_transpose_reindex_shared_dyn_warp = T.allocate([1024], "float16", "warp")
                    for ax0_0 in range(T.int64(4)):
                        T.ptx_ldmatrix("float16", T.bool(False), 4, ".b16", A_reindex_shared_dyn_warp, ax0_0 * T.int64(256) + threadIdx_x * T.int64(8), T.tvm_access_ptr(T.type_annotation("float16"), A_reindex_shared_dyn, ax3_0_0 % T.int64(4) * T.int64(4096) + threadIdx_z * T.int64(2048) + ax0_0 * T.int64(512) + threadIdx_x % T.int64(16) * T.int64(32) + T.bitwise_xor(ax3_0_1 * T.int64(2) + threadIdx_x // T.int64(16), threadIdx_x % T.int64(8) // T.int64(2)) * T.int64(8), T.int64(512), 1), T.int64(0))
                    for ax0_0 in range(T.int64(4)):
                        T.ptx_ldmatrix("float16", T.bool(False), 4, ".b16", T_transpose_reindex_shared_dyn_warp, ax0_0 * T.int64(256) + threadIdx_x * T.int64(8), T.tvm_access_ptr(T.type_annotation("float16"), T_transpose_reindex_shared_dyn, ax3_0_0 % T.int64(4) * T.int64(4096) + threadIdx_y * T.int64(2048) + ax0_0 * T.int64(512) + threadIdx_x // T.int64(16) * T.int64(256) + threadIdx_x % T.int64(8) * T.int64(32) + T.bitwise_xor(ax3_0_1 * T.int64(2) + threadIdx_x % T.int64(16) // T.int64(8), threadIdx_x % T.int64(8) // T.int64(2)) * T.int64(8), T.int64(512), 1), T.int64(0))
                    for ax1_0_3, ax2_0_3 in T.grid(4, 4):
                        T.ptx_mma("float32", "m16n8k16", "row", "col", "fp16", "fp16", "fp32", A_reindex_shared_dyn_warp, T.Cast("int64", ax1_0_3) * T.int64(256) + threadIdx_x * T.int64(8), T_transpose_reindex_shared_dyn_warp, T.Cast("int64", ax2_0_3) * T.int64(256) + threadIdx_x * T.int64(8), matmul_reindex_shared_dyn_warp, T.Cast("int64", ax1_0_3) * T.int64(1024) + T.Cast("int64", ax2_0_3) * T.int64(256) + threadIdx_x * T.int64(8), T.bool(False))
                        T.ptx_mma("float32", "m16n8k16", "row", "col", "fp16", "fp16", "fp32", A_reindex_shared_dyn_warp, T.Cast("int64", ax1_0_3) * T.int64(256) + threadIdx_x * T.int64(8), T_transpose_reindex_shared_dyn_warp, T.Cast("int64", ax2_0_3) * T.int64(256) + threadIdx_x * T.int64(8) + T.int64(4), matmul_reindex_shared_dyn_warp, T.Cast("int64", ax1_0_3) * T.int64(1024) + T.Cast("int64", ax2_0_3) * T.int64(256) + threadIdx_x * T.int64(8) + T.int64(4), T.bool(False))
            for ax3_0_0 in T.unroll(T.int64(3)):
                T.attr(0, "async_wait_queue_scope", 0)
                T.attr(0, "async_wait_inflight_count", T.int64(2) - ax3_0_0)
                for ax3_0_1 in range(T.int64(2)):
                    A_reindex_shared_dyn_warp = T.allocate([1024], "float16", "warp")
                    T_transpose_reindex_shared_dyn_warp = T.allocate([1024], "float16", "warp")
                    for ax0_0 in range(T.int64(4)):
                        T.ptx_ldmatrix("float16", T.bool(False), 4, ".b16", A_reindex_shared_dyn_warp, ax0_0 * T.int64(256) + threadIdx_x * T.int64(8), T.tvm_access_ptr(T.type_annotation("float16"), A_reindex_shared_dyn, ax3_0_0 * T.int64(4096) + threadIdx_z * T.int64(2048) + ax0_0 * T.int64(512) + threadIdx_x % T.int64(16) * T.int64(32) + T.bitwise_xor(ax3_0_1 * T.int64(2) + threadIdx_x // T.int64(16), threadIdx_x % T.int64(8) // T.int64(2)) * T.int64(8) + T.int64(4096), T.int64(512), 1), T.int64(0))
                    for ax0_0 in range(T.int64(4)):
                        T.ptx_ldmatrix("float16", T.bool(False), 4, ".b16", T_transpose_reindex_shared_dyn_warp, ax0_0 * T.int64(256) + threadIdx_x * T.int64(8), T.tvm_access_ptr(T.type_annotation("float16"), T_transpose_reindex_shared_dyn, ax3_0_0 * T.int64(4096) + threadIdx_y * T.int64(2048) + ax0_0 * T.int64(512) + threadIdx_x // T.int64(16) * T.int64(256) + threadIdx_x % T.int64(8) * T.int64(32) + T.bitwise_xor(ax3_0_1 * T.int64(2) + threadIdx_x % T.int64(16) // T.int64(8), threadIdx_x % T.int64(8) // T.int64(2)) * T.int64(8) + T.int64(4096), T.int64(512), 1), T.int64(0))
                    for ax1_0_3, ax2_0_3 in T.grid(4, 4):
                        T.ptx_mma("float32", "m16n8k16", "row", "col", "fp16", "fp16", "fp32", A_reindex_shared_dyn_warp, T.Cast("int64", ax1_0_3) * T.int64(256) + threadIdx_x * T.int64(8), T_transpose_reindex_shared_dyn_warp, T.Cast("int64", ax2_0_3) * T.int64(256) + threadIdx_x * T.int64(8), matmul_reindex_shared_dyn_warp, T.Cast("int64", ax1_0_3) * T.int64(1024) + T.Cast("int64", ax2_0_3) * T.int64(256) + threadIdx_x * T.int64(8), T.bool(False))
                        T.ptx_mma("float32", "m16n8k16", "row", "col", "fp16", "fp16", "fp32", A_reindex_shared_dyn_warp, T.Cast("int64", ax1_0_3) * T.int64(256) + threadIdx_x * T.int64(8), T_transpose_reindex_shared_dyn_warp, T.Cast("int64", ax2_0_3) * T.int64(256) + threadIdx_x * T.int64(8) + T.int64(4), matmul_reindex_shared_dyn_warp, T.Cast("int64", ax1_0_3) * T.int64(1024) + T.Cast("int64", ax2_0_3) * T.int64(256) + threadIdx_x * T.int64(8) + T.int64(4), T.bool(False))
        p_output0_intermediate_reindex_shared_dyn = T.Buffer((T.int64(16384),), data=matmul_reindex_shared_dyn, scope="shared.dyn")
        for ax1_1, ax2_1, local_id in T.grid(T.int64(4), T.int64(4), T.int64(8)):
            p_output0_intermediate_reindex_shared_dyn_warp = T.Buffer((T.int64(4096),), data=matmul_reindex_shared_dyn_warp, scope="warp")
            p_output0_intermediate_reindex_shared_dyn[threadIdx_z * T.int64(8192) + ax1_1 * T.int64(2048) + local_id % T.int64(4) // T.int64(2) * T.int64(1024) + threadIdx_x // T.int64(4) * T.int64(128) + T.bitwise_xor(threadIdx_y * T.int64(8) + ax2_1 * T.int64(2) + local_id // T.int64(4), threadIdx_x // T.int64(4)) * T.int64(8) + threadIdx_x % T.int64(4) * T.int64(2) + local_id % T.int64(2)] = p_output0_intermediate_reindex_shared_dyn_warp[ax1_1 * T.int64(1024) + ax2_1 * T.int64(256) + threadIdx_x * T.int64(8) + local_id]
        for ax0_ax1_fused_0 in range(T.int64(16)):
            var_compute_intermediate_1 = T.Buffer((T.int64(16777216),), "float16", data=var_compute_intermediate.data)
            var_compute_intermediate_1[T.Cast("int64", blockIdx_x) // T.int64(32) * T.int64(524288) + ax0_ax1_fused_0 * T.int64(32768) + threadIdx_z * T.int64(16384) + threadIdx_y * T.int64(8192) + threadIdx_x // T.int64(16) * T.int64(4096) + T.Cast("int64", blockIdx_x) % T.int64(32) * T.int64(128) + threadIdx_x % T.int64(16) * T.int64(8):T.Cast("int64", blockIdx_x) // T.int64(32) * T.int64(524288) + ax0_ax1_fused_0 * T.int64(32768) + threadIdx_z * T.int64(16384) + threadIdx_y * T.int64(8192) + threadIdx_x // T.int64(16) * T.int64(4096) + T.Cast("int64", blockIdx_x) % T.int64(32) * T.int64(128) + threadIdx_x % T.int64(16) * T.int64(8) + T.int64(8)] = T.Cast("float16x8", p_output0_intermediate_reindex_shared_dyn[ax0_ax1_fused_0 * T.int64(1024) + threadIdx_z * T.int64(512) + threadIdx_y * T.int64(256) + threadIdx_x // T.int64(16) * T.int64(128) + T.bitwise_xor(threadIdx_x % T.int64(16), threadIdx_z * T.int64(4) + threadIdx_y * T.int64(2) + threadIdx_x // T.int64(16)) * T.int64(8):ax0_ax1_fused_0 * T.int64(1024) + threadIdx_z * T.int64(512) + threadIdx_y * T.int64(256) + threadIdx_x // T.int64(16) * T.int64(128) + T.bitwise_xor(threadIdx_x % T.int64(16), threadIdx_z * T.int64(4) + threadIdx_y * T.int64(2) + threadIdx_x // T.int64(16)) * T.int64(8) + T.int64(8)])
