#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
#include <cuda_fp16.h>
__device__ half max(half a, half b)
{
  return __hgt(__half(a), __half(b)) ? a : b;
}
__device__ half min(half a, half b)
{
  return __hlt(__half(a), __half(b)) ? a : b;
}
#else

typedef unsigned short uint16_t;
typedef unsigned char uint8_t;
typedef signed char int8_t;
typedef int int32_t;
typedef unsigned long long uint64_t;
typedef unsigned int uint32_t;

#define TVM_FORCE_INLINE inline __attribute__((always_inline))
#define TVM_XINLINE TVM_FORCE_INLINE __device__ __host__
#define TVM_ALIGNED(x) __attribute__ ((aligned(x)))
#define TVM_HALF_OPERATOR(RTYPE, OP)                              \
  TVM_XINLINE RTYPE operator OP (half a, half b) {                \
    return RTYPE(float(a) OP float(b));                           \
  }                                                               \
  template<typename T>                                            \
  TVM_XINLINE RTYPE operator OP (half a, T b) {                   \
    return RTYPE(float(a) OP float(b));                           \
  }                                                               \
  template<typename T>                                            \
  TVM_XINLINE RTYPE operator OP (T a, half b) {                   \
    return RTYPE(float(a) OP float(b));                           \
  }

#define TVM_HALF_ASSIGNOP(AOP, OP)                                \
  template<typename T>                                            \
  TVM_XINLINE half operator AOP (const T& a) {                    \
    return *this = half(float(*this) OP float(a));                \
  }                                                               \
  template<typename T>                                            \
  TVM_XINLINE half operator AOP (const volatile T& a) volatile {  \
    return *this = half(float(*this) OP float(a));                \
  }

class TVM_ALIGNED(2) half {
 public:
  uint16_t half_;

  static TVM_XINLINE half Binary(uint16_t value) {
    half res;
    res.half_ = value;
    return res;
  }

  TVM_XINLINE half() {}

  TVM_XINLINE half(const float& value) { constructor(value); }
  TVM_XINLINE explicit half(const double& value) { constructor(value); }
  TVM_XINLINE explicit half(const int8_t& value) { constructor(value); }
  TVM_XINLINE explicit half(const uint8_t& value) { constructor(value); }
  TVM_XINLINE explicit half(const int32_t& value) { constructor(value); }
  TVM_XINLINE explicit half(const uint32_t& value) { constructor(value); }
  TVM_XINLINE explicit half(const long long& value) { constructor(value); }
  TVM_XINLINE explicit half(const uint64_t& value) { constructor(value); }

  TVM_XINLINE operator float() const {                          \
    return float(half2float(half_));                            \
  }                                                             \
  TVM_XINLINE operator float() const volatile {                 \
    return float(half2float(half_));                            \
  }


  TVM_HALF_ASSIGNOP(+=, +)
  TVM_HALF_ASSIGNOP(-=, -)
  TVM_HALF_ASSIGNOP(*=, *)
  TVM_HALF_ASSIGNOP(/=, /)

  TVM_XINLINE half operator+() {
    return *this;
  }

  TVM_XINLINE half operator-() {
    return half(-float(*this));
  }

  TVM_XINLINE half operator=(const half& a) {
    half_ = a.half_;
    return a;
  }

  template<typename T>
  TVM_XINLINE half operator=(const T& a) {
    return *this = half(a);
  }

  TVM_XINLINE half operator=(const half& a) volatile {
    half_ = a.half_;
    return a;
  }

  template<typename T>
  TVM_XINLINE half operator=(const T& a) volatile {
    return *this = half(a);
  }

 private:
  union Bits {
    float f;
    int32_t si;
    uint32_t ui;
  };

  static int const fp16FractionBits = 10;
  static int const fp32FractionBits = 23;
  static int32_t const fp32FractionMask = ~(~0u << fp32FractionBits);   // == 0x7fffff
  static int32_t const fp32HiddenBit = 1 << fp32FractionBits;   // == 0x800000
  static int const shift = fp32FractionBits - fp16FractionBits;   // == 13
  static int const shiftSign = 16;
  static int32_t const expAdjust = 127 - 15;   // exp32-127 = exp16-15, so exp16 = exp32 - (127-15)

  static int32_t const infN = 0x7F800000;   // flt32 infinity
  static int32_t const maxN = 0x477FFFFF;   // max flt32 that's a flt16 normal after >> by shift
  static int32_t const minN = 0x38800000;   // min flt16 normal as a flt32
  static int32_t const maxZ = 0x33000000;   // max fp32 number that's still rounded to zero in fp16
  static int32_t const signN = 0x80000000;  // flt32 sign bit

  static int32_t const infC = infN >> shift;
  static int32_t const nanN = (infC + 1) << shift;   // minimum flt16 nan as a flt32
  static int32_t const maxC = maxN >> shift;
  static int32_t const minC = minN >> shift;
  static int32_t const signC = signN >> shiftSign;  // flt16 sign bit

  static int32_t const mulN = 0x52000000;  // (1 << 23) / minN
  static int32_t const mulC = 0x33800000;  // minN / (1 << (23 - shift))

  static int32_t const subC = 0x003FF;  // max flt32 subnormal down shifted
  static int32_t const norC = 0x00400;  // min flt32 normal down shifted

  static int32_t const maxD = infC - maxC - 1;
  static int32_t const minD = minC - subC - 1;

  TVM_XINLINE uint16_t float2half(const float& value) const {
    Bits v;
    v.f = value;
    uint32_t sign = v.si & signN;    // grab sign bit
    v.si ^= sign;                    // clear sign bit from v
    sign >>= shiftSign;              // logical shift sign to fp16 position

    if (v.si <= maxZ) {
      // Handle eventual zeros here to ensure
      // vshift will not exceed 32 below.
      v.ui = 0;
    } else if (v.si < minN) {
      // Handle denorms
      uint32_t exp32 = v.ui >> fp32FractionBits;
      int32_t exp16 = exp32 - expAdjust;
      // If exp16 == 0 (just into the denorm range), then significant should be shifted right 1.
      // Smaller (so negative) exp16 values should result in greater right shifts.
      uint32_t vshift = 1 - exp16;
      uint32_t significand = fp32HiddenBit | (v.ui & fp32FractionMask);
      v.ui = significand >> vshift;
      v.ui += (v.ui & 0x3fff) != 0x1000 || (significand & 0x7ff) ? 0x1000 : 0;
    } else if (v.si <= maxN) {
      // Handle norms
      v.ui += (v.ui & 0x3fff) != 0x1000 ? 0x1000 : 0;
      v.ui -= expAdjust << fp32FractionBits;
    } else if (v.si <= infN) {
      v.si = infN;
    } else if (v.si < nanN) {
      v.si = nanN;
    }

    v.ui >>= shift;
    return sign | (v.ui & 0x7fff);
  }

  // Same as above routine, except for addition of volatile keyword
  TVM_XINLINE uint16_t float2half(
    const volatile float& value) const volatile {
    Bits v;
    v.f = value;
    uint32_t sign = v.si & signN;    // grab sign bit
    v.si ^= sign;                    // clear sign bit from v
    sign >>= shiftSign;              // logical shift sign to fp16 position

    if (v.si <= maxZ) {
      // Handle eventual zeros here to ensure
      // vshift will not exceed 32 below.
      v.ui = 0;
    } else if (v.si < minN) {
      // Handle denorms
      uint32_t exp32 = v.ui >> fp32FractionBits;
      int32_t exp16 = exp32 - expAdjust;
      // If exp16 == 0 (just into the denorm range), then significant should be shifted right 1.
      // Smaller (so negative) exp16 values should result in greater right shifts.
      uint32_t vshift = 1 - exp16;
      uint32_t significand = fp32HiddenBit | (v.ui & fp32FractionMask);
      v.ui = significand >> vshift;
      v.ui += (v.ui & 0x3fff) != 0x1000 || (significand & 0x7ff) ? 0x1000 : 0;
    } else if (v.si <= maxN) {
      // Handle norms
      v.ui += (v.ui & 0x3fff) != 0x1000 ? 0x1000 : 0;
      v.ui -= expAdjust << fp32FractionBits;
    } else if (v.si <= infN) {
      v.si = infN;
    } else if (v.si < nanN) {
      v.si = nanN;
    }

    v.ui >>= shift;
    return sign | (v.ui & 0x7fff);
  }

  TVM_XINLINE float half2float(const uint16_t& value) const {
    Bits v;
    v.ui = value;
    int32_t sign = v.si & signC;
    v.si ^= sign;
    sign <<= shiftSign;
    v.si ^= ((v.si + minD) ^ v.si) & -(v.si > subC);
    v.si ^= ((v.si + maxD) ^ v.si) & -(v.si > maxC);
    Bits s;
    s.si = mulC;
    s.f *= v.si;
    int32_t mask = -(norC > v.si);
    v.si <<= shift;
    v.si ^= (s.si ^ v.si) & mask;
    v.si |= sign;
    return v.f;
  }

  TVM_XINLINE float half2float(
    const volatile uint16_t& value) const volatile {
    Bits v;
    v.ui = value;
    int32_t sign = v.si & signC;
    v.si ^= sign;
    sign <<= shiftSign;
    v.si ^= ((v.si + minD) ^ v.si) & -(v.si > subC);
    v.si ^= ((v.si + maxD) ^ v.si) & -(v.si > maxC);
    Bits s;
    s.si = mulC;
    s.f *= v.si;
    int32_t mask = -(norC > v.si);
    v.si <<= shift;
    v.si ^= (s.si ^ v.si) & mask;
    v.si |= sign;
    return v.f;
  }

  template<typename T>
  TVM_XINLINE void constructor(const T& value) {
    half_ = float2half(float(value));
  }
};

TVM_HALF_OPERATOR(half, +)
TVM_HALF_OPERATOR(half, -)
TVM_HALF_OPERATOR(half, *)
TVM_HALF_OPERATOR(half, /)
TVM_HALF_OPERATOR(bool, >)
TVM_HALF_OPERATOR(bool, <)
TVM_HALF_OPERATOR(bool, >=)
TVM_HALF_OPERATOR(bool, <=)

TVM_XINLINE half __float2half_rn(const float a) {
  return half(a);
}
#endif


// Pack two half values.
static inline __device__ __host__ unsigned
__pack_half2(const half x, const half y) {
  unsigned v0 = *((unsigned short *)&x);
  unsigned v1 = *((unsigned short *)&y);
  return (v1 << 16) | v0;
}

#define CUDA_UNSUPPORTED_HALF_MATH_BINARY(HALF_MATH_NAME, FP32_MATH_NAME) \
static inline __device__ __host__ half HALF_MATH_NAME(half x, half y) {   \
  float tmp_x = __half2float(x);                                          \
  float tmp_y = __half2float(y);                                          \
  float result = FP32_MATH_NAME(tmp_x, tmp_y);                            \
  return __float2half(result);                                            \
}

#define CUDA_UNSUPPORTED_HALF_MATH_UNARY(HALF_MATH_NAME, FP32_MATH_NAME) \
static inline __device__ __host__ half HALF_MATH_NAME(half x) {          \
  float tmp_x = __half2float(x);                                         \
  float result = FP32_MATH_NAME(tmp_x);                                  \
  return __float2half(result);                                           \
}

// Some fp16 math functions are not supported in cuda_fp16.h,
// so we define them here to make sure the generated CUDA code
// is valid.
#if defined(__CUDA_ARCH__)
#if (__CUDA_ARCH__ >= 530)
CUDA_UNSUPPORTED_HALF_MATH_BINARY(hpow, powf)
CUDA_UNSUPPORTED_HALF_MATH_UNARY(htanh, tanhf)
CUDA_UNSUPPORTED_HALF_MATH_UNARY(htan, tanf)
CUDA_UNSUPPORTED_HALF_MATH_UNARY(hatan, atanf)
CUDA_UNSUPPORTED_HALF_MATH_UNARY(herf, erf)
#else
CUDA_UNSUPPORTED_HALF_MATH_UNARY(hexp, exp)
#endif
#endif

#undef CUDA_UNSUPPORTED_HALF_MATH_BINARY
#undef CUDA_UNSUPPORTED_HALF_MATH_UNARY
__forceinline__ __device__ unsigned int
cast_smem_ptr_to_int(const void* const smem_ptr)
{
  unsigned int smem_int;
  asm volatile ("{ .reg .u64 smem_int; cvta.to.shared.u64 smem_int, %1; cvt.u32.u64 %0, smem_int; }"
    : "=r"(smem_int) : "l"(smem_ptr));
  return smem_int;
}

#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \
     (__CUDACC_VER_MAJOR__ > 11))
#define TVM_ENABLE_L2_PREFETCH 1
#else
#define TVM_ENABLE_L2_PREFETCH 0
#endif

#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(half* __restrict__ A, half* __restrict__ B, half* __restrict__ compute) {
  extern __shared__ uchar buf_dyn_shmem[];
  float matmul_reindex_shared_dyn_warp[128];
  half A_reindex_shared_dyn_warp[32];
  half T_transpose_reindex_shared_dyn_warp[32];
  half A_reindex_shared_dyn_warp_1[32];
  half T_transpose_reindex_shared_dyn_warp_1[32];
  for (int ax1_0_3_init = 0; ax1_0_3_init < 4; ++ax1_0_3_init) {
    for (int ax2_0_3_init = 0; ax2_0_3_init < 4; ++ax2_0_3_init) {
      for (int i = 0; i < 8; ++i) {
matmul_reindex_shared_dyn_warp[((ax1_0_3_init * 32) + (ax2_0_3_init * 8)) + i] = 0.0;}
;
    }
  }
  for (int64_t ax0_ax1_fused_0 = 0; ax0_ax1_fused_0 < (int64_t)4; ++ax0_ax1_fused_0) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + (((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_1 = 0; ax0_ax1_fused_0_1 < (int64_t)4; ++ax0_ax1_fused_0_1) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_1 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + (((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_1 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

  for (int64_t ax0_ax1_fused_0_2 = 0; ax0_ax1_fused_0_2 < (int64_t)4; ++ax0_ax1_fused_0_2) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_2 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_2 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)32))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_3 = 0; ax0_ax1_fused_0_3 < (int64_t)4; ++ax0_ax1_fused_0_3) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_3 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_3 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)32))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

  for (int64_t ax0_ax1_fused_0_4 = 0; ax0_ax1_fused_0_4 < (int64_t)4; ++ax0_ax1_fused_0_4) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_4 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_4 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)64))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_5 = 0; ax0_ax1_fused_0_5 < (int64_t)4; ++ax0_ax1_fused_0_5) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_5 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_5 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)64))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

  for (int64_t ax0_ax1_fused_0_6 = 0; ax0_ax1_fused_0_6 < (int64_t)4; ++ax0_ax1_fused_0_6) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_6 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_6 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)96))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_7 = 0; ax0_ax1_fused_0_7 < (int64_t)4; ++ax0_ax1_fused_0_7) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_7 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_7 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)96))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1 = 0; ax3_0_1 < (int64_t)2; ++ax3_0_1) {
    for (int64_t ax0_0 = 0; ax0_0 < (int64_t)4; ++ax0_0) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_1 = 0; ax0_0_1 < (int64_t)4; ++ax0_0_1) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_1 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_1 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_1 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_1 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_1 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3 = 0; ax1_0_3 < 4; ++ax1_0_3) {
      for (int ax2_0_3 = 0; ax2_0_3 < 4; ++ax2_0_3) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3) * (int64_t)32) + (((int64_t)ax2_0_3) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_8 = 0; ax0_ax1_fused_0_8 < (int64_t)4; ++ax0_ax1_fused_0_8) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_8 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_8 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)128))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_9 = 0; ax0_ax1_fused_0_9 < (int64_t)4; ++ax0_ax1_fused_0_9) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_9 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_9 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)128))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_1 = 0; ax3_0_1_1 < (int64_t)2; ++ax3_0_1_1) {
    for (int64_t ax0_0_2 = 0; ax0_0_2 < (int64_t)4; ++ax0_0_2) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_2 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_1 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_2 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_2 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_2 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_2 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_3 = 0; ax0_0_3 < (int64_t)4; ++ax0_0_3) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_3 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_1 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_3 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_3 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_3 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_3 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_1 = 0; ax1_0_3_1 < 4; ++ax1_0_3_1) {
      for (int ax2_0_3_1 = 0; ax2_0_3_1 < 4; ++ax2_0_3_1) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_1) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_1) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_1) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_1) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_1) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_1) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_1) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_1) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_1) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_1) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_1) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_1) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_1) * (int64_t)32) + (((int64_t)ax2_0_3_1) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_10 = 0; ax0_ax1_fused_0_10 < (int64_t)4; ++ax0_ax1_fused_0_10) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_10 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_10 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)160))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_11 = 0; ax0_ax1_fused_0_11 < (int64_t)4; ++ax0_ax1_fused_0_11) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_11 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_11 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)160))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_2 = 0; ax3_0_1_2 < (int64_t)2; ++ax3_0_1_2) {
    for (int64_t ax0_0_4 = 0; ax0_0_4 < (int64_t)4; ++ax0_0_4) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_4 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_2 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_4 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_4 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_4 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_4 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_5 = 0; ax0_0_5 < (int64_t)4; ++ax0_0_5) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_5 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_2 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_5 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_5 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_5 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_5 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_2 = 0; ax1_0_3_2 < 4; ++ax1_0_3_2) {
      for (int ax2_0_3_2 = 0; ax2_0_3_2 < 4; ++ax2_0_3_2) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_2) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_2) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_2) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_2) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_2) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_2) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_2) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_2) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_2) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_2) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_2) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_2) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_2) * (int64_t)32) + (((int64_t)ax2_0_3_2) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_12 = 0; ax0_ax1_fused_0_12 < (int64_t)4; ++ax0_ax1_fused_0_12) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_12 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_12 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)192))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_13 = 0; ax0_ax1_fused_0_13 < (int64_t)4; ++ax0_ax1_fused_0_13) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_13 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_13 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)192))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_3 = 0; ax3_0_1_3 < (int64_t)2; ++ax3_0_1_3) {
    for (int64_t ax0_0_6 = 0; ax0_0_6 < (int64_t)4; ++ax0_0_6) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_6 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_3 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_6 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_6 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_6 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_6 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_7 = 0; ax0_0_7 < (int64_t)4; ++ax0_0_7) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_7 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_3 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_7 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_7 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_7 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_7 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_3 = 0; ax1_0_3_3 < 4; ++ax1_0_3_3) {
      for (int ax2_0_3_3 = 0; ax2_0_3_3 < 4; ++ax2_0_3_3) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_3) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_3) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_3) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_3) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_3) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_3) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_3) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_3) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_3) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_3) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_3) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_3) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_3) * (int64_t)32) + (((int64_t)ax2_0_3_3) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_14 = 0; ax0_ax1_fused_0_14 < (int64_t)4; ++ax0_ax1_fused_0_14) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_14 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_14 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)224))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_15 = 0; ax0_ax1_fused_0_15 < (int64_t)4; ++ax0_ax1_fused_0_15) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_15 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_15 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)224))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_4 = 0; ax3_0_1_4 < (int64_t)2; ++ax3_0_1_4) {
    for (int64_t ax0_0_8 = 0; ax0_0_8 < (int64_t)4; ++ax0_0_8) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_8 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_4 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_8 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_8 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_8 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_8 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_9 = 0; ax0_0_9 < (int64_t)4; ++ax0_0_9) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_9 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_4 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_9 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_9 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_9 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_9 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_4 = 0; ax1_0_3_4 < 4; ++ax1_0_3_4) {
      for (int ax2_0_3_4 = 0; ax2_0_3_4 < 4; ++ax2_0_3_4) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_4) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_4) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_4) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_4) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_4) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_4) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_4) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_4) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_4) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_4) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_4) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_4) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_4) * (int64_t)32) + (((int64_t)ax2_0_3_4) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_16 = 0; ax0_ax1_fused_0_16 < (int64_t)4; ++ax0_ax1_fused_0_16) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_16 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_16 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)256))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_17 = 0; ax0_ax1_fused_0_17 < (int64_t)4; ++ax0_ax1_fused_0_17) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_17 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_17 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)256))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_5 = 0; ax3_0_1_5 < (int64_t)2; ++ax3_0_1_5) {
    for (int64_t ax0_0_10 = 0; ax0_0_10 < (int64_t)4; ++ax0_0_10) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_10 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_5 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_10 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_10 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_10 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_10 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_11 = 0; ax0_0_11 < (int64_t)4; ++ax0_0_11) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_11 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_5 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_11 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_11 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_11 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_11 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_5 = 0; ax1_0_3_5 < 4; ++ax1_0_3_5) {
      for (int ax2_0_3_5 = 0; ax2_0_3_5 < 4; ++ax2_0_3_5) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_5) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_5) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_5) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_5) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_5) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_5) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_5) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_5) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_5) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_5) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_5) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_5) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_5) * (int64_t)32) + (((int64_t)ax2_0_3_5) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_18 = 0; ax0_ax1_fused_0_18 < (int64_t)4; ++ax0_ax1_fused_0_18) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_18 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_18 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)288))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_19 = 0; ax0_ax1_fused_0_19 < (int64_t)4; ++ax0_ax1_fused_0_19) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_19 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_19 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)288))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_6 = 0; ax3_0_1_6 < (int64_t)2; ++ax3_0_1_6) {
    for (int64_t ax0_0_12 = 0; ax0_0_12 < (int64_t)4; ++ax0_0_12) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_12 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_6 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_12 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_12 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_12 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_12 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_13 = 0; ax0_0_13 < (int64_t)4; ++ax0_0_13) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_13 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_6 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_13 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_13 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_13 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_13 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_6 = 0; ax1_0_3_6 < 4; ++ax1_0_3_6) {
      for (int ax2_0_3_6 = 0; ax2_0_3_6 < 4; ++ax2_0_3_6) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_6) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_6) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_6) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_6) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_6) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_6) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_6) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_6) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_6) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_6) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_6) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_6) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_6) * (int64_t)32) + (((int64_t)ax2_0_3_6) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_20 = 0; ax0_ax1_fused_0_20 < (int64_t)4; ++ax0_ax1_fused_0_20) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_20 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_20 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)320))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_21 = 0; ax0_ax1_fused_0_21 < (int64_t)4; ++ax0_ax1_fused_0_21) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_21 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_21 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)320))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_7 = 0; ax3_0_1_7 < (int64_t)2; ++ax3_0_1_7) {
    for (int64_t ax0_0_14 = 0; ax0_0_14 < (int64_t)4; ++ax0_0_14) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_14 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_7 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_14 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_14 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_14 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_14 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_15 = 0; ax0_0_15 < (int64_t)4; ++ax0_0_15) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_15 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_7 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_15 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_15 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_15 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_15 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_7 = 0; ax1_0_3_7 < 4; ++ax1_0_3_7) {
      for (int ax2_0_3_7 = 0; ax2_0_3_7 < 4; ++ax2_0_3_7) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_7) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_7) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_7) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_7) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_7) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_7) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_7) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_7) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_7) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_7) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_7) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_7) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_7) * (int64_t)32) + (((int64_t)ax2_0_3_7) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_22 = 0; ax0_ax1_fused_0_22 < (int64_t)4; ++ax0_ax1_fused_0_22) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_22 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_22 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)352))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_23 = 0; ax0_ax1_fused_0_23 < (int64_t)4; ++ax0_ax1_fused_0_23) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_23 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_23 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)352))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_8 = 0; ax3_0_1_8 < (int64_t)2; ++ax3_0_1_8) {
    for (int64_t ax0_0_16 = 0; ax0_0_16 < (int64_t)4; ++ax0_0_16) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_16 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_8 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_16 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_16 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_16 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_16 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_17 = 0; ax0_0_17 < (int64_t)4; ++ax0_0_17) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_17 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_8 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_17 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_17 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_17 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_17 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_8 = 0; ax1_0_3_8 < 4; ++ax1_0_3_8) {
      for (int ax2_0_3_8 = 0; ax2_0_3_8 < 4; ++ax2_0_3_8) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_8) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_8) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_8) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_8) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_8) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_8) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_8) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_8) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_8) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_8) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_8) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_8) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_8) * (int64_t)32) + (((int64_t)ax2_0_3_8) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_24 = 0; ax0_ax1_fused_0_24 < (int64_t)4; ++ax0_ax1_fused_0_24) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_24 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_24 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)384))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_25 = 0; ax0_ax1_fused_0_25 < (int64_t)4; ++ax0_ax1_fused_0_25) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_25 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_25 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)384))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_9 = 0; ax3_0_1_9 < (int64_t)2; ++ax3_0_1_9) {
    for (int64_t ax0_0_18 = 0; ax0_0_18 < (int64_t)4; ++ax0_0_18) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_18 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_9 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_18 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_18 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_18 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_18 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_19 = 0; ax0_0_19 < (int64_t)4; ++ax0_0_19) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_19 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_9 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_19 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_19 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_19 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_19 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_9 = 0; ax1_0_3_9 < 4; ++ax1_0_3_9) {
      for (int ax2_0_3_9 = 0; ax2_0_3_9 < 4; ++ax2_0_3_9) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_9) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_9) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_9) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_9) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_9) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_9) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_9) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_9) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_9) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_9) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_9) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_9) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_9) * (int64_t)32) + (((int64_t)ax2_0_3_9) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_26 = 0; ax0_ax1_fused_0_26 < (int64_t)4; ++ax0_ax1_fused_0_26) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_26 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_26 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)416))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_27 = 0; ax0_ax1_fused_0_27 < (int64_t)4; ++ax0_ax1_fused_0_27) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_27 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_27 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)416))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_10 = 0; ax3_0_1_10 < (int64_t)2; ++ax3_0_1_10) {
    for (int64_t ax0_0_20 = 0; ax0_0_20 < (int64_t)4; ++ax0_0_20) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_20 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_10 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_20 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_20 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_20 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_20 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_21 = 0; ax0_0_21 < (int64_t)4; ++ax0_0_21) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_21 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_10 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_21 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_21 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_21 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_21 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_10 = 0; ax1_0_3_10 < 4; ++ax1_0_3_10) {
      for (int ax2_0_3_10 = 0; ax2_0_3_10 < 4; ++ax2_0_3_10) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_10) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_10) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_10) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_10) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_10) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_10) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_10) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_10) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_10) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_10) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_10) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_10) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_10) * (int64_t)32) + (((int64_t)ax2_0_3_10) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_28 = 0; ax0_ax1_fused_0_28 < (int64_t)4; ++ax0_ax1_fused_0_28) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_28 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_28 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)448))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_29 = 0; ax0_ax1_fused_0_29 < (int64_t)4; ++ax0_ax1_fused_0_29) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_29 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_29 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)448))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_11 = 0; ax3_0_1_11 < (int64_t)2; ++ax3_0_1_11) {
    for (int64_t ax0_0_22 = 0; ax0_0_22 < (int64_t)4; ++ax0_0_22) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_22 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_11 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_22 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_22 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_22 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_22 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_23 = 0; ax0_0_23 < (int64_t)4; ++ax0_0_23) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_23 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_11 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_23 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_23 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_23 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_23 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_11 = 0; ax1_0_3_11 < 4; ++ax1_0_3_11) {
      for (int ax2_0_3_11 = 0; ax2_0_3_11 < 4; ++ax2_0_3_11) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_11) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_11) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_11) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_11) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_11) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_11) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_11) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_11) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_11) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_11) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_11) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_11) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_11) * (int64_t)32) + (((int64_t)ax2_0_3_11) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_30 = 0; ax0_ax1_fused_0_30 < (int64_t)4; ++ax0_ax1_fused_0_30) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_30 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_30 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)480))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_31 = 0; ax0_ax1_fused_0_31 < (int64_t)4; ++ax0_ax1_fused_0_31) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_31 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_31 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)480))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_12 = 0; ax3_0_1_12 < (int64_t)2; ++ax3_0_1_12) {
    for (int64_t ax0_0_24 = 0; ax0_0_24 < (int64_t)4; ++ax0_0_24) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_24 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_12 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_24 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_24 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_24 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_24 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_25 = 0; ax0_0_25 < (int64_t)4; ++ax0_0_25) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_25 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_12 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_25 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_25 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_25 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_25 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_12 = 0; ax1_0_3_12 < 4; ++ax1_0_3_12) {
      for (int ax2_0_3_12 = 0; ax2_0_3_12 < 4; ++ax2_0_3_12) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_12) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_12) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_12) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_12) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_12) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_12) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_12) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_12) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_12) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_12) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_12) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_12) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_12) * (int64_t)32) + (((int64_t)ax2_0_3_12) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_32 = 0; ax0_ax1_fused_0_32 < (int64_t)4; ++ax0_ax1_fused_0_32) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_32 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_32 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)512))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_33 = 0; ax0_ax1_fused_0_33 < (int64_t)4; ++ax0_ax1_fused_0_33) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_33 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_33 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)512))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_13 = 0; ax3_0_1_13 < (int64_t)2; ++ax3_0_1_13) {
    for (int64_t ax0_0_26 = 0; ax0_0_26 < (int64_t)4; ++ax0_0_26) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_26 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_13 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_26 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_26 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_26 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_26 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_27 = 0; ax0_0_27 < (int64_t)4; ++ax0_0_27) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_27 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_13 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_27 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_27 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_27 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_27 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_13 = 0; ax1_0_3_13 < 4; ++ax1_0_3_13) {
      for (int ax2_0_3_13 = 0; ax2_0_3_13 < 4; ++ax2_0_3_13) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_13) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_13) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_13) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_13) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_13) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_13) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_13) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_13) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_13) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_13) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_13) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_13) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_13) * (int64_t)32) + (((int64_t)ax2_0_3_13) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_34 = 0; ax0_ax1_fused_0_34 < (int64_t)4; ++ax0_ax1_fused_0_34) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_34 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_34 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)544))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_35 = 0; ax0_ax1_fused_0_35 < (int64_t)4; ++ax0_ax1_fused_0_35) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_35 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_35 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)544))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_14 = 0; ax3_0_1_14 < (int64_t)2; ++ax3_0_1_14) {
    for (int64_t ax0_0_28 = 0; ax0_0_28 < (int64_t)4; ++ax0_0_28) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_28 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_14 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_28 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_28 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_28 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_28 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_29 = 0; ax0_0_29 < (int64_t)4; ++ax0_0_29) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_29 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_14 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_29 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_29 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_29 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_29 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_14 = 0; ax1_0_3_14 < 4; ++ax1_0_3_14) {
      for (int ax2_0_3_14 = 0; ax2_0_3_14 < 4; ++ax2_0_3_14) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_14) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_14) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_14) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_14) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_14) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_14) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_14) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_14) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_14) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_14) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_14) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_14) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_14) * (int64_t)32) + (((int64_t)ax2_0_3_14) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_36 = 0; ax0_ax1_fused_0_36 < (int64_t)4; ++ax0_ax1_fused_0_36) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_36 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_36 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)576))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_37 = 0; ax0_ax1_fused_0_37 < (int64_t)4; ++ax0_ax1_fused_0_37) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_37 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_37 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)576))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_15 = 0; ax3_0_1_15 < (int64_t)2; ++ax3_0_1_15) {
    for (int64_t ax0_0_30 = 0; ax0_0_30 < (int64_t)4; ++ax0_0_30) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_30 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_15 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_30 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_30 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_30 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_30 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_31 = 0; ax0_0_31 < (int64_t)4; ++ax0_0_31) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_31 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_15 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_31 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_31 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_31 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_31 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_15 = 0; ax1_0_3_15 < 4; ++ax1_0_3_15) {
      for (int ax2_0_3_15 = 0; ax2_0_3_15 < 4; ++ax2_0_3_15) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_15) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_15) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_15) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_15) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_15) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_15) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_15) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_15) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_15) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_15) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_15) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_15) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_15) * (int64_t)32) + (((int64_t)ax2_0_3_15) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_38 = 0; ax0_ax1_fused_0_38 < (int64_t)4; ++ax0_ax1_fused_0_38) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_38 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_38 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)608))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_39 = 0; ax0_ax1_fused_0_39 < (int64_t)4; ++ax0_ax1_fused_0_39) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_39 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_39 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)608))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_16 = 0; ax3_0_1_16 < (int64_t)2; ++ax3_0_1_16) {
    for (int64_t ax0_0_32 = 0; ax0_0_32 < (int64_t)4; ++ax0_0_32) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_32 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_16 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_32 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_32 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_32 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_32 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_33 = 0; ax0_0_33 < (int64_t)4; ++ax0_0_33) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_33 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_16 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_33 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_33 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_33 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_33 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_16 = 0; ax1_0_3_16 < 4; ++ax1_0_3_16) {
      for (int ax2_0_3_16 = 0; ax2_0_3_16 < 4; ++ax2_0_3_16) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_16) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_16) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_16) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_16) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_16) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_16) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_16) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_16) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_16) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_16) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_16) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_16) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_16) * (int64_t)32) + (((int64_t)ax2_0_3_16) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_40 = 0; ax0_ax1_fused_0_40 < (int64_t)4; ++ax0_ax1_fused_0_40) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_40 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_40 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)640))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_41 = 0; ax0_ax1_fused_0_41 < (int64_t)4; ++ax0_ax1_fused_0_41) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_41 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_41 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)640))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_17 = 0; ax3_0_1_17 < (int64_t)2; ++ax3_0_1_17) {
    for (int64_t ax0_0_34 = 0; ax0_0_34 < (int64_t)4; ++ax0_0_34) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_34 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_17 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_34 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_34 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_34 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_34 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_35 = 0; ax0_0_35 < (int64_t)4; ++ax0_0_35) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_35 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_17 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_35 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_35 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_35 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_35 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_17 = 0; ax1_0_3_17 < 4; ++ax1_0_3_17) {
      for (int ax2_0_3_17 = 0; ax2_0_3_17 < 4; ++ax2_0_3_17) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_17) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_17) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_17) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_17) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_17) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_17) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_17) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_17) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_17) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_17) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_17) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_17) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_17) * (int64_t)32) + (((int64_t)ax2_0_3_17) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_42 = 0; ax0_ax1_fused_0_42 < (int64_t)4; ++ax0_ax1_fused_0_42) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_42 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_42 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)672))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_43 = 0; ax0_ax1_fused_0_43 < (int64_t)4; ++ax0_ax1_fused_0_43) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_43 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_43 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)672))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_18 = 0; ax3_0_1_18 < (int64_t)2; ++ax3_0_1_18) {
    for (int64_t ax0_0_36 = 0; ax0_0_36 < (int64_t)4; ++ax0_0_36) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_36 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_18 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_36 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_36 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_36 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_36 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_37 = 0; ax0_0_37 < (int64_t)4; ++ax0_0_37) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_37 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_18 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_37 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_37 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_37 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_37 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_18 = 0; ax1_0_3_18 < 4; ++ax1_0_3_18) {
      for (int ax2_0_3_18 = 0; ax2_0_3_18 < 4; ++ax2_0_3_18) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_18) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_18) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_18) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_18) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_18) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_18) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_18) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_18) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_18) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_18) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_18) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_18) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_18) * (int64_t)32) + (((int64_t)ax2_0_3_18) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_44 = 0; ax0_ax1_fused_0_44 < (int64_t)4; ++ax0_ax1_fused_0_44) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_44 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_44 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)704))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_45 = 0; ax0_ax1_fused_0_45 < (int64_t)4; ++ax0_ax1_fused_0_45) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_45 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_45 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)704))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_19 = 0; ax3_0_1_19 < (int64_t)2; ++ax3_0_1_19) {
    for (int64_t ax0_0_38 = 0; ax0_0_38 < (int64_t)4; ++ax0_0_38) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_38 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_19 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_38 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_38 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_38 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_38 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_39 = 0; ax0_0_39 < (int64_t)4; ++ax0_0_39) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_39 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_19 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_39 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_39 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_39 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_39 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_19 = 0; ax1_0_3_19 < 4; ++ax1_0_3_19) {
      for (int ax2_0_3_19 = 0; ax2_0_3_19 < 4; ++ax2_0_3_19) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_19) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_19) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_19) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_19) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_19) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_19) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_19) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_19) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_19) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_19) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_19) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_19) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_19) * (int64_t)32) + (((int64_t)ax2_0_3_19) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_46 = 0; ax0_ax1_fused_0_46 < (int64_t)4; ++ax0_ax1_fused_0_46) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_46 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_46 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)736))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_47 = 0; ax0_ax1_fused_0_47 < (int64_t)4; ++ax0_ax1_fused_0_47) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_47 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_47 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)736))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_20 = 0; ax3_0_1_20 < (int64_t)2; ++ax3_0_1_20) {
    for (int64_t ax0_0_40 = 0; ax0_0_40 < (int64_t)4; ++ax0_0_40) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_40 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_20 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_40 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_40 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_40 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_40 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_41 = 0; ax0_0_41 < (int64_t)4; ++ax0_0_41) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_41 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_20 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_41 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_41 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_41 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_41 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_20 = 0; ax1_0_3_20 < 4; ++ax1_0_3_20) {
      for (int ax2_0_3_20 = 0; ax2_0_3_20 < 4; ++ax2_0_3_20) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_20) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_20) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_20) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_20) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_20) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_20) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_20) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_20) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_20) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_20) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_20) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_20) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_20) * (int64_t)32) + (((int64_t)ax2_0_3_20) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_48 = 0; ax0_ax1_fused_0_48 < (int64_t)4; ++ax0_ax1_fused_0_48) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_48 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_48 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)768))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_49 = 0; ax0_ax1_fused_0_49 < (int64_t)4; ++ax0_ax1_fused_0_49) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_49 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_49 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)768))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_21 = 0; ax3_0_1_21 < (int64_t)2; ++ax3_0_1_21) {
    for (int64_t ax0_0_42 = 0; ax0_0_42 < (int64_t)4; ++ax0_0_42) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_42 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_21 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_42 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_42 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_42 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_42 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_43 = 0; ax0_0_43 < (int64_t)4; ++ax0_0_43) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_43 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_21 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_43 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_43 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_43 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_43 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_21 = 0; ax1_0_3_21 < 4; ++ax1_0_3_21) {
      for (int ax2_0_3_21 = 0; ax2_0_3_21 < 4; ++ax2_0_3_21) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_21) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_21) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_21) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_21) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_21) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_21) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_21) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_21) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_21) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_21) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_21) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_21) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_21) * (int64_t)32) + (((int64_t)ax2_0_3_21) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_50 = 0; ax0_ax1_fused_0_50 < (int64_t)4; ++ax0_ax1_fused_0_50) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_50 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_50 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)800))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_51 = 0; ax0_ax1_fused_0_51 < (int64_t)4; ++ax0_ax1_fused_0_51) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_51 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_51 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)800))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_22 = 0; ax3_0_1_22 < (int64_t)2; ++ax3_0_1_22) {
    for (int64_t ax0_0_44 = 0; ax0_0_44 < (int64_t)4; ++ax0_0_44) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_44 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_22 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_44 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_44 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_44 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_44 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_45 = 0; ax0_0_45 < (int64_t)4; ++ax0_0_45) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_45 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_22 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_45 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_45 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_45 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_45 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_22 = 0; ax1_0_3_22 < 4; ++ax1_0_3_22) {
      for (int ax2_0_3_22 = 0; ax2_0_3_22 < 4; ++ax2_0_3_22) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_22) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_22) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_22) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_22) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_22) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_22) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_22) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_22) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_22) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_22) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_22) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_22) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_22) * (int64_t)32) + (((int64_t)ax2_0_3_22) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_52 = 0; ax0_ax1_fused_0_52 < (int64_t)4; ++ax0_ax1_fused_0_52) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_52 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_52 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)832))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_53 = 0; ax0_ax1_fused_0_53 < (int64_t)4; ++ax0_ax1_fused_0_53) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_53 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_53 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)832))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_23 = 0; ax3_0_1_23 < (int64_t)2; ++ax3_0_1_23) {
    for (int64_t ax0_0_46 = 0; ax0_0_46 < (int64_t)4; ++ax0_0_46) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_46 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_23 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_46 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_46 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_46 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_46 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_47 = 0; ax0_0_47 < (int64_t)4; ++ax0_0_47) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_47 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_23 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_47 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_47 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_47 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_47 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_23 = 0; ax1_0_3_23 < 4; ++ax1_0_3_23) {
      for (int ax2_0_3_23 = 0; ax2_0_3_23 < 4; ++ax2_0_3_23) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_23) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_23) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_23) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_23) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_23) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_23) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_23) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_23) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_23) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_23) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_23) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_23) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_23) * (int64_t)32) + (((int64_t)ax2_0_3_23) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_54 = 0; ax0_ax1_fused_0_54 < (int64_t)4; ++ax0_ax1_fused_0_54) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_54 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_54 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)864))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_55 = 0; ax0_ax1_fused_0_55 < (int64_t)4; ++ax0_ax1_fused_0_55) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_55 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_55 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)864))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_24 = 0; ax3_0_1_24 < (int64_t)2; ++ax3_0_1_24) {
    for (int64_t ax0_0_48 = 0; ax0_0_48 < (int64_t)4; ++ax0_0_48) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_48 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_24 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_48 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_48 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_48 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_48 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_49 = 0; ax0_0_49 < (int64_t)4; ++ax0_0_49) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_49 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_24 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_49 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_49 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_49 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_49 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_24 = 0; ax1_0_3_24 < 4; ++ax1_0_3_24) {
      for (int ax2_0_3_24 = 0; ax2_0_3_24 < 4; ++ax2_0_3_24) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_24) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_24) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_24) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_24) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_24) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_24) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_24) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_24) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_24) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_24) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_24) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_24) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_24) * (int64_t)32) + (((int64_t)ax2_0_3_24) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_56 = 0; ax0_ax1_fused_0_56 < (int64_t)4; ++ax0_ax1_fused_0_56) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_56 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_56 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)896))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_57 = 0; ax0_ax1_fused_0_57 < (int64_t)4; ++ax0_ax1_fused_0_57) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_57 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_57 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)896))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_25 = 0; ax3_0_1_25 < (int64_t)2; ++ax3_0_1_25) {
    for (int64_t ax0_0_50 = 0; ax0_0_50 < (int64_t)4; ++ax0_0_50) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_50 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_25 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_50 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_50 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_50 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_50 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_51 = 0; ax0_0_51 < (int64_t)4; ++ax0_0_51) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_51 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_25 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_51 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_51 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_51 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_51 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_25 = 0; ax1_0_3_25 < 4; ++ax1_0_3_25) {
      for (int ax2_0_3_25 = 0; ax2_0_3_25 < 4; ++ax2_0_3_25) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_25) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_25) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_25) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_25) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_25) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_25) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_25) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_25) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_25) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_25) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_25) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_25) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_25) * (int64_t)32) + (((int64_t)ax2_0_3_25) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_58 = 0; ax0_ax1_fused_0_58 < (int64_t)4; ++ax0_ax1_fused_0_58) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_58 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_58 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)928))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_59 = 0; ax0_ax1_fused_0_59 < (int64_t)4; ++ax0_ax1_fused_0_59) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_59 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_59 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)928))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_26 = 0; ax3_0_1_26 < (int64_t)2; ++ax3_0_1_26) {
    for (int64_t ax0_0_52 = 0; ax0_0_52 < (int64_t)4; ++ax0_0_52) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_52 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_26 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_52 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_52 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_52 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_52 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_53 = 0; ax0_0_53 < (int64_t)4; ++ax0_0_53) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_53 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_26 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_53 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_53 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_53 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_53 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_26 = 0; ax1_0_3_26 < 4; ++ax1_0_3_26) {
      for (int ax2_0_3_26 = 0; ax2_0_3_26 < 4; ++ax2_0_3_26) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_26) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_26) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_26) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_26) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_26) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_26) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_26) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_26) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_26) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_26) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_26) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_26) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_26) * (int64_t)32) + (((int64_t)ax2_0_3_26) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_60 = 0; ax0_ax1_fused_0_60 < (int64_t)4; ++ax0_ax1_fused_0_60) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_60 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_60 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)960))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_61 = 0; ax0_ax1_fused_0_61 < (int64_t)4; ++ax0_ax1_fused_0_61) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_61 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_61 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)960))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_27 = 0; ax3_0_1_27 < (int64_t)2; ++ax3_0_1_27) {
    for (int64_t ax0_0_54 = 0; ax0_0_54 < (int64_t)4; ++ax0_0_54) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_54 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_27 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_54 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_54 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_54 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_54 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_55 = 0; ax0_0_55 < (int64_t)4; ++ax0_0_55) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_55 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_27 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_55 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_55 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_55 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_55 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_27 = 0; ax1_0_3_27 < 4; ++ax1_0_3_27) {
      for (int ax2_0_3_27 = 0; ax2_0_3_27 < 4; ++ax2_0_3_27) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_27) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_27) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_27) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_27) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_27) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_27) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_27) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_27) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_27) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_27) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_27) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_27) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_27) * (int64_t)32) + (((int64_t)ax2_0_3_27) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_62 = 0; ax0_ax1_fused_0_62 < (int64_t)4; ++ax0_ax1_fused_0_62) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_62 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_62 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)992))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_63 = 0; ax0_ax1_fused_0_63 < (int64_t)4; ++ax0_ax1_fused_0_63) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_63 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_63 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)992))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_28 = 0; ax3_0_1_28 < (int64_t)2; ++ax3_0_1_28) {
    for (int64_t ax0_0_56 = 0; ax0_0_56 < (int64_t)4; ++ax0_0_56) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_56 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_28 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_56 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_56 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_56 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_56 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_57 = 0; ax0_0_57 < (int64_t)4; ++ax0_0_57) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_57 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_28 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_57 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_57 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_57 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_57 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_28 = 0; ax1_0_3_28 < 4; ++ax1_0_3_28) {
      for (int ax2_0_3_28 = 0; ax2_0_3_28 < 4; ++ax2_0_3_28) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_28) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_28) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_28) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_28) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_28) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_28) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_28) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_28) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_28) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_28) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_28) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_28) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_28) * (int64_t)32) + (((int64_t)ax2_0_3_28) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_64 = 0; ax0_ax1_fused_0_64 < (int64_t)4; ++ax0_ax1_fused_0_64) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_64 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_64 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1024))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_65 = 0; ax0_ax1_fused_0_65 < (int64_t)4; ++ax0_ax1_fused_0_65) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_65 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_65 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1024))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_29 = 0; ax3_0_1_29 < (int64_t)2; ++ax3_0_1_29) {
    for (int64_t ax0_0_58 = 0; ax0_0_58 < (int64_t)4; ++ax0_0_58) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_58 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_29 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_58 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_58 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_58 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_58 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_59 = 0; ax0_0_59 < (int64_t)4; ++ax0_0_59) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_59 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_29 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_59 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_59 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_59 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_59 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_29 = 0; ax1_0_3_29 < 4; ++ax1_0_3_29) {
      for (int ax2_0_3_29 = 0; ax2_0_3_29 < 4; ++ax2_0_3_29) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_29) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_29) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_29) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_29) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_29) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_29) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_29) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_29) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_29) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_29) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_29) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_29) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_29) * (int64_t)32) + (((int64_t)ax2_0_3_29) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_66 = 0; ax0_ax1_fused_0_66 < (int64_t)4; ++ax0_ax1_fused_0_66) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_66 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_66 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1056))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_67 = 0; ax0_ax1_fused_0_67 < (int64_t)4; ++ax0_ax1_fused_0_67) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_67 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_67 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1056))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_30 = 0; ax3_0_1_30 < (int64_t)2; ++ax3_0_1_30) {
    for (int64_t ax0_0_60 = 0; ax0_0_60 < (int64_t)4; ++ax0_0_60) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_60 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_30 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_60 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_60 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_60 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_60 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_61 = 0; ax0_0_61 < (int64_t)4; ++ax0_0_61) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_61 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_30 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_61 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_61 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_61 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_61 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_30 = 0; ax1_0_3_30 < 4; ++ax1_0_3_30) {
      for (int ax2_0_3_30 = 0; ax2_0_3_30 < 4; ++ax2_0_3_30) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_30) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_30) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_30) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_30) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_30) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_30) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_30) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_30) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_30) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_30) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_30) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_30) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_30) * (int64_t)32) + (((int64_t)ax2_0_3_30) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_68 = 0; ax0_ax1_fused_0_68 < (int64_t)4; ++ax0_ax1_fused_0_68) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_68 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_68 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1088))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_69 = 0; ax0_ax1_fused_0_69 < (int64_t)4; ++ax0_ax1_fused_0_69) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_69 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_69 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1088))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_31 = 0; ax3_0_1_31 < (int64_t)2; ++ax3_0_1_31) {
    for (int64_t ax0_0_62 = 0; ax0_0_62 < (int64_t)4; ++ax0_0_62) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_62 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_31 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_62 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_62 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_62 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_62 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_63 = 0; ax0_0_63 < (int64_t)4; ++ax0_0_63) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_63 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_31 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_63 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_63 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_63 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_63 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_31 = 0; ax1_0_3_31 < 4; ++ax1_0_3_31) {
      for (int ax2_0_3_31 = 0; ax2_0_3_31 < 4; ++ax2_0_3_31) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_31) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_31) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_31) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_31) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_31) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_31) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_31) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_31) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_31) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_31) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_31) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_31) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_31) * (int64_t)32) + (((int64_t)ax2_0_3_31) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_70 = 0; ax0_ax1_fused_0_70 < (int64_t)4; ++ax0_ax1_fused_0_70) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_70 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_70 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1120))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_71 = 0; ax0_ax1_fused_0_71 < (int64_t)4; ++ax0_ax1_fused_0_71) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_71 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_71 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1120))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_32 = 0; ax3_0_1_32 < (int64_t)2; ++ax3_0_1_32) {
    for (int64_t ax0_0_64 = 0; ax0_0_64 < (int64_t)4; ++ax0_0_64) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_64 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_32 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_64 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_64 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_64 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_64 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_65 = 0; ax0_0_65 < (int64_t)4; ++ax0_0_65) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_65 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_32 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_65 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_65 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_65 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_65 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_32 = 0; ax1_0_3_32 < 4; ++ax1_0_3_32) {
      for (int ax2_0_3_32 = 0; ax2_0_3_32 < 4; ++ax2_0_3_32) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_32) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_32) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_32) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_32) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_32) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_32) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_32) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_32) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_32) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_32) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_32) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_32) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_32) * (int64_t)32) + (((int64_t)ax2_0_3_32) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_72 = 0; ax0_ax1_fused_0_72 < (int64_t)4; ++ax0_ax1_fused_0_72) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_72 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_72 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1152))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_73 = 0; ax0_ax1_fused_0_73 < (int64_t)4; ++ax0_ax1_fused_0_73) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_73 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_73 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1152))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_33 = 0; ax3_0_1_33 < (int64_t)2; ++ax3_0_1_33) {
    for (int64_t ax0_0_66 = 0; ax0_0_66 < (int64_t)4; ++ax0_0_66) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_66 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_33 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_66 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_66 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_66 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_66 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_67 = 0; ax0_0_67 < (int64_t)4; ++ax0_0_67) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_67 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_33 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_67 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_67 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_67 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_67 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_33 = 0; ax1_0_3_33 < 4; ++ax1_0_3_33) {
      for (int ax2_0_3_33 = 0; ax2_0_3_33 < 4; ++ax2_0_3_33) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_33) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_33) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_33) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_33) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_33) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_33) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_33) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_33) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_33) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_33) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_33) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_33) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_33) * (int64_t)32) + (((int64_t)ax2_0_3_33) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_74 = 0; ax0_ax1_fused_0_74 < (int64_t)4; ++ax0_ax1_fused_0_74) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_74 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_74 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1184))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_75 = 0; ax0_ax1_fused_0_75 < (int64_t)4; ++ax0_ax1_fused_0_75) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_75 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_75 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1184))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_34 = 0; ax3_0_1_34 < (int64_t)2; ++ax3_0_1_34) {
    for (int64_t ax0_0_68 = 0; ax0_0_68 < (int64_t)4; ++ax0_0_68) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_68 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_34 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_68 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_68 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_68 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_68 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_69 = 0; ax0_0_69 < (int64_t)4; ++ax0_0_69) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_69 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_34 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_69 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_69 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_69 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_69 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_34 = 0; ax1_0_3_34 < 4; ++ax1_0_3_34) {
      for (int ax2_0_3_34 = 0; ax2_0_3_34 < 4; ++ax2_0_3_34) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_34) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_34) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_34) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_34) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_34) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_34) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_34) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_34) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_34) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_34) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_34) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_34) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_34) * (int64_t)32) + (((int64_t)ax2_0_3_34) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_76 = 0; ax0_ax1_fused_0_76 < (int64_t)4; ++ax0_ax1_fused_0_76) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_76 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_76 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1216))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_77 = 0; ax0_ax1_fused_0_77 < (int64_t)4; ++ax0_ax1_fused_0_77) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_77 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_77 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1216))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_35 = 0; ax3_0_1_35 < (int64_t)2; ++ax3_0_1_35) {
    for (int64_t ax0_0_70 = 0; ax0_0_70 < (int64_t)4; ++ax0_0_70) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_70 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_35 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_70 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_70 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_70 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_70 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_71 = 0; ax0_0_71 < (int64_t)4; ++ax0_0_71) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_71 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_35 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_71 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_71 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_71 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_71 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_35 = 0; ax1_0_3_35 < 4; ++ax1_0_3_35) {
      for (int ax2_0_3_35 = 0; ax2_0_3_35 < 4; ++ax2_0_3_35) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_35) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_35) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_35) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_35) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_35) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_35) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_35) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_35) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_35) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_35) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_35) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_35) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_35) * (int64_t)32) + (((int64_t)ax2_0_3_35) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_78 = 0; ax0_ax1_fused_0_78 < (int64_t)4; ++ax0_ax1_fused_0_78) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_78 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_78 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1248))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_79 = 0; ax0_ax1_fused_0_79 < (int64_t)4; ++ax0_ax1_fused_0_79) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_79 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_79 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1248))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_36 = 0; ax3_0_1_36 < (int64_t)2; ++ax3_0_1_36) {
    for (int64_t ax0_0_72 = 0; ax0_0_72 < (int64_t)4; ++ax0_0_72) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_72 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_36 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_72 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_72 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_72 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_72 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_73 = 0; ax0_0_73 < (int64_t)4; ++ax0_0_73) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_73 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_36 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_73 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_73 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_73 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_73 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_36 = 0; ax1_0_3_36 < 4; ++ax1_0_3_36) {
      for (int ax2_0_3_36 = 0; ax2_0_3_36 < 4; ++ax2_0_3_36) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_36) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_36) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_36) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_36) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_36) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_36) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_36) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_36) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_36) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_36) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_36) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_36) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_36) * (int64_t)32) + (((int64_t)ax2_0_3_36) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_80 = 0; ax0_ax1_fused_0_80 < (int64_t)4; ++ax0_ax1_fused_0_80) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_80 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_80 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1280))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_81 = 0; ax0_ax1_fused_0_81 < (int64_t)4; ++ax0_ax1_fused_0_81) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_81 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_81 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1280))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_37 = 0; ax3_0_1_37 < (int64_t)2; ++ax3_0_1_37) {
    for (int64_t ax0_0_74 = 0; ax0_0_74 < (int64_t)4; ++ax0_0_74) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_74 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_37 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_74 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_74 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_74 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_74 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_75 = 0; ax0_0_75 < (int64_t)4; ++ax0_0_75) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_75 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_37 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_75 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_75 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_75 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_75 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_37 = 0; ax1_0_3_37 < 4; ++ax1_0_3_37) {
      for (int ax2_0_3_37 = 0; ax2_0_3_37 < 4; ++ax2_0_3_37) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_37) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_37) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_37) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_37) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_37) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_37) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_37) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_37) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_37) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_37) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_37) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_37) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_37) * (int64_t)32) + (((int64_t)ax2_0_3_37) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_82 = 0; ax0_ax1_fused_0_82 < (int64_t)4; ++ax0_ax1_fused_0_82) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_82 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_82 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1312))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_83 = 0; ax0_ax1_fused_0_83 < (int64_t)4; ++ax0_ax1_fused_0_83) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_83 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_83 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1312))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_38 = 0; ax3_0_1_38 < (int64_t)2; ++ax3_0_1_38) {
    for (int64_t ax0_0_76 = 0; ax0_0_76 < (int64_t)4; ++ax0_0_76) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_76 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_38 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_76 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_76 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_76 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_76 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_77 = 0; ax0_0_77 < (int64_t)4; ++ax0_0_77) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_77 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_38 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_77 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_77 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_77 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_77 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_38 = 0; ax1_0_3_38 < 4; ++ax1_0_3_38) {
      for (int ax2_0_3_38 = 0; ax2_0_3_38 < 4; ++ax2_0_3_38) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_38) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_38) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_38) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_38) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_38) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_38) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_38) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_38) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_38) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_38) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_38) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_38) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_38) * (int64_t)32) + (((int64_t)ax2_0_3_38) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_84 = 0; ax0_ax1_fused_0_84 < (int64_t)4; ++ax0_ax1_fused_0_84) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_84 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_84 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1344))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_85 = 0; ax0_ax1_fused_0_85 < (int64_t)4; ++ax0_ax1_fused_0_85) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_85 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_85 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1344))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_39 = 0; ax3_0_1_39 < (int64_t)2; ++ax3_0_1_39) {
    for (int64_t ax0_0_78 = 0; ax0_0_78 < (int64_t)4; ++ax0_0_78) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_78 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_39 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_78 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_78 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_78 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_78 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_79 = 0; ax0_0_79 < (int64_t)4; ++ax0_0_79) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_79 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_39 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_79 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_79 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_79 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_79 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_39 = 0; ax1_0_3_39 < 4; ++ax1_0_3_39) {
      for (int ax2_0_3_39 = 0; ax2_0_3_39 < 4; ++ax2_0_3_39) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_39) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_39) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_39) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_39) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_39) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_39) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_39) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_39) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_39) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_39) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_39) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_39) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_39) * (int64_t)32) + (((int64_t)ax2_0_3_39) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_86 = 0; ax0_ax1_fused_0_86 < (int64_t)4; ++ax0_ax1_fused_0_86) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_86 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_86 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1376))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_87 = 0; ax0_ax1_fused_0_87 < (int64_t)4; ++ax0_ax1_fused_0_87) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_87 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_87 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1376))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_40 = 0; ax3_0_1_40 < (int64_t)2; ++ax3_0_1_40) {
    for (int64_t ax0_0_80 = 0; ax0_0_80 < (int64_t)4; ++ax0_0_80) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_80 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_40 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_80 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_80 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_80 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_80 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_81 = 0; ax0_0_81 < (int64_t)4; ++ax0_0_81) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_81 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_40 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_81 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_81 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_81 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_81 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_40 = 0; ax1_0_3_40 < 4; ++ax1_0_3_40) {
      for (int ax2_0_3_40 = 0; ax2_0_3_40 < 4; ++ax2_0_3_40) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_40) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_40) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_40) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_40) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_40) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_40) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_40) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_40) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_40) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_40) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_40) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_40) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_40) * (int64_t)32) + (((int64_t)ax2_0_3_40) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_88 = 0; ax0_ax1_fused_0_88 < (int64_t)4; ++ax0_ax1_fused_0_88) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_88 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_88 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1408))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_89 = 0; ax0_ax1_fused_0_89 < (int64_t)4; ++ax0_ax1_fused_0_89) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_89 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_89 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1408))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_41 = 0; ax3_0_1_41 < (int64_t)2; ++ax3_0_1_41) {
    for (int64_t ax0_0_82 = 0; ax0_0_82 < (int64_t)4; ++ax0_0_82) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_82 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_41 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_82 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_82 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_82 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_82 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_83 = 0; ax0_0_83 < (int64_t)4; ++ax0_0_83) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_83 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_41 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_83 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_83 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_83 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_83 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_41 = 0; ax1_0_3_41 < 4; ++ax1_0_3_41) {
      for (int ax2_0_3_41 = 0; ax2_0_3_41 < 4; ++ax2_0_3_41) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_41) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_41) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_41) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_41) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_41) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_41) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_41) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_41) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_41) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_41) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_41) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_41) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_41) * (int64_t)32) + (((int64_t)ax2_0_3_41) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_90 = 0; ax0_ax1_fused_0_90 < (int64_t)4; ++ax0_ax1_fused_0_90) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_90 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_90 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1440))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_91 = 0; ax0_ax1_fused_0_91 < (int64_t)4; ++ax0_ax1_fused_0_91) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_91 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_91 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1440))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_42 = 0; ax3_0_1_42 < (int64_t)2; ++ax3_0_1_42) {
    for (int64_t ax0_0_84 = 0; ax0_0_84 < (int64_t)4; ++ax0_0_84) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_84 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_42 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_84 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_84 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_84 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_84 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_85 = 0; ax0_0_85 < (int64_t)4; ++ax0_0_85) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_85 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_42 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_85 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_85 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_85 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_85 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_42 = 0; ax1_0_3_42 < 4; ++ax1_0_3_42) {
      for (int ax2_0_3_42 = 0; ax2_0_3_42 < 4; ++ax2_0_3_42) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_42) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_42) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_42) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_42) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_42) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_42) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_42) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_42) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_42) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_42) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_42) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_42) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_42) * (int64_t)32) + (((int64_t)ax2_0_3_42) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_92 = 0; ax0_ax1_fused_0_92 < (int64_t)4; ++ax0_ax1_fused_0_92) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_92 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_92 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1472))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_93 = 0; ax0_ax1_fused_0_93 < (int64_t)4; ++ax0_ax1_fused_0_93) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_93 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_93 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1472))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_43 = 0; ax3_0_1_43 < (int64_t)2; ++ax3_0_1_43) {
    for (int64_t ax0_0_86 = 0; ax0_0_86 < (int64_t)4; ++ax0_0_86) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_86 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_43 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_86 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_86 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_86 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_86 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_87 = 0; ax0_0_87 < (int64_t)4; ++ax0_0_87) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_87 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_43 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_87 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_87 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_87 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_87 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_43 = 0; ax1_0_3_43 < 4; ++ax1_0_3_43) {
      for (int ax2_0_3_43 = 0; ax2_0_3_43 < 4; ++ax2_0_3_43) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_43) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_43) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_43) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_43) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_43) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_43) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_43) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_43) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_43) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_43) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_43) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_43) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_43) * (int64_t)32) + (((int64_t)ax2_0_3_43) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_94 = 0; ax0_ax1_fused_0_94 < (int64_t)4; ++ax0_ax1_fused_0_94) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_94 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_94 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1504))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_95 = 0; ax0_ax1_fused_0_95 < (int64_t)4; ++ax0_ax1_fused_0_95) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_95 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_95 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1504))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_44 = 0; ax3_0_1_44 < (int64_t)2; ++ax3_0_1_44) {
    for (int64_t ax0_0_88 = 0; ax0_0_88 < (int64_t)4; ++ax0_0_88) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_88 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_44 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_88 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_88 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_88 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_88 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_89 = 0; ax0_0_89 < (int64_t)4; ++ax0_0_89) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_89 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_44 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_89 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_89 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_89 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_89 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_44 = 0; ax1_0_3_44 < 4; ++ax1_0_3_44) {
      for (int ax2_0_3_44 = 0; ax2_0_3_44 < 4; ++ax2_0_3_44) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_44) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_44) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_44) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_44) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_44) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_44) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_44) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_44) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_44) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_44) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_44) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_44) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_44) * (int64_t)32) + (((int64_t)ax2_0_3_44) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_96 = 0; ax0_ax1_fused_0_96 < (int64_t)4; ++ax0_ax1_fused_0_96) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_96 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_96 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1536))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_97 = 0; ax0_ax1_fused_0_97 < (int64_t)4; ++ax0_ax1_fused_0_97) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_97 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_97 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1536))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_45 = 0; ax3_0_1_45 < (int64_t)2; ++ax3_0_1_45) {
    for (int64_t ax0_0_90 = 0; ax0_0_90 < (int64_t)4; ++ax0_0_90) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_90 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_45 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_90 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_90 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_90 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_90 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_91 = 0; ax0_0_91 < (int64_t)4; ++ax0_0_91) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_91 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_45 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_91 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_91 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_91 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_91 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_45 = 0; ax1_0_3_45 < 4; ++ax1_0_3_45) {
      for (int ax2_0_3_45 = 0; ax2_0_3_45 < 4; ++ax2_0_3_45) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_45) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_45) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_45) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_45) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_45) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_45) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_45) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_45) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_45) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_45) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_45) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_45) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_45) * (int64_t)32) + (((int64_t)ax2_0_3_45) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_98 = 0; ax0_ax1_fused_0_98 < (int64_t)4; ++ax0_ax1_fused_0_98) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_98 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_98 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1568))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_99 = 0; ax0_ax1_fused_0_99 < (int64_t)4; ++ax0_ax1_fused_0_99) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_99 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_99 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1568))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_46 = 0; ax3_0_1_46 < (int64_t)2; ++ax3_0_1_46) {
    for (int64_t ax0_0_92 = 0; ax0_0_92 < (int64_t)4; ++ax0_0_92) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_92 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_46 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_92 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_92 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_92 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_92 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_93 = 0; ax0_0_93 < (int64_t)4; ++ax0_0_93) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_93 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_46 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_93 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_93 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_93 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_93 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_46 = 0; ax1_0_3_46 < 4; ++ax1_0_3_46) {
      for (int ax2_0_3_46 = 0; ax2_0_3_46 < 4; ++ax2_0_3_46) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_46) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_46) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_46) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_46) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_46) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_46) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_46) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_46) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_46) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_46) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_46) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_46) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_46) * (int64_t)32) + (((int64_t)ax2_0_3_46) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_100 = 0; ax0_ax1_fused_0_100 < (int64_t)4; ++ax0_ax1_fused_0_100) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_100 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_100 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1600))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_101 = 0; ax0_ax1_fused_0_101 < (int64_t)4; ++ax0_ax1_fused_0_101) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_101 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_101 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1600))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_47 = 0; ax3_0_1_47 < (int64_t)2; ++ax3_0_1_47) {
    for (int64_t ax0_0_94 = 0; ax0_0_94 < (int64_t)4; ++ax0_0_94) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_94 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_47 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_94 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_94 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_94 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_94 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_95 = 0; ax0_0_95 < (int64_t)4; ++ax0_0_95) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_95 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_47 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_95 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_95 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_95 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_95 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_47 = 0; ax1_0_3_47 < 4; ++ax1_0_3_47) {
      for (int ax2_0_3_47 = 0; ax2_0_3_47 < 4; ++ax2_0_3_47) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_47) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_47) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_47) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_47) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_47) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_47) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_47) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_47) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_47) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_47) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_47) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_47) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_47) * (int64_t)32) + (((int64_t)ax2_0_3_47) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_102 = 0; ax0_ax1_fused_0_102 < (int64_t)4; ++ax0_ax1_fused_0_102) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_102 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_102 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1632))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_103 = 0; ax0_ax1_fused_0_103 < (int64_t)4; ++ax0_ax1_fused_0_103) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_103 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_103 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1632))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_48 = 0; ax3_0_1_48 < (int64_t)2; ++ax3_0_1_48) {
    for (int64_t ax0_0_96 = 0; ax0_0_96 < (int64_t)4; ++ax0_0_96) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_96 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_48 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_96 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_96 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_96 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_96 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_97 = 0; ax0_0_97 < (int64_t)4; ++ax0_0_97) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_97 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_48 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_97 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_97 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_97 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_97 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_48 = 0; ax1_0_3_48 < 4; ++ax1_0_3_48) {
      for (int ax2_0_3_48 = 0; ax2_0_3_48 < 4; ++ax2_0_3_48) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_48) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_48) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_48) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_48) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_48) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_48) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_48) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_48) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_48) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_48) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_48) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_48) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_48) * (int64_t)32) + (((int64_t)ax2_0_3_48) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_104 = 0; ax0_ax1_fused_0_104 < (int64_t)4; ++ax0_ax1_fused_0_104) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_104 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_104 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1664))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_105 = 0; ax0_ax1_fused_0_105 < (int64_t)4; ++ax0_ax1_fused_0_105) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_105 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_105 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1664))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_49 = 0; ax3_0_1_49 < (int64_t)2; ++ax3_0_1_49) {
    for (int64_t ax0_0_98 = 0; ax0_0_98 < (int64_t)4; ++ax0_0_98) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_98 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_49 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_98 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_98 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_98 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_98 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_99 = 0; ax0_0_99 < (int64_t)4; ++ax0_0_99) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_99 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_49 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_99 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_99 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_99 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_99 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_49 = 0; ax1_0_3_49 < 4; ++ax1_0_3_49) {
      for (int ax2_0_3_49 = 0; ax2_0_3_49 < 4; ++ax2_0_3_49) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_49) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_49) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_49) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_49) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_49) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_49) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_49) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_49) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_49) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_49) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_49) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_49) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_49) * (int64_t)32) + (((int64_t)ax2_0_3_49) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_106 = 0; ax0_ax1_fused_0_106 < (int64_t)4; ++ax0_ax1_fused_0_106) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_106 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_106 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1696))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_107 = 0; ax0_ax1_fused_0_107 < (int64_t)4; ++ax0_ax1_fused_0_107) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_107 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_107 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1696))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_50 = 0; ax3_0_1_50 < (int64_t)2; ++ax3_0_1_50) {
    for (int64_t ax0_0_100 = 0; ax0_0_100 < (int64_t)4; ++ax0_0_100) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_100 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_50 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_100 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_100 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_100 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_100 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_101 = 0; ax0_0_101 < (int64_t)4; ++ax0_0_101) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_101 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_50 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_101 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_101 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_101 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_101 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_50 = 0; ax1_0_3_50 < 4; ++ax1_0_3_50) {
      for (int ax2_0_3_50 = 0; ax2_0_3_50 < 4; ++ax2_0_3_50) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_50) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_50) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_50) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_50) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_50) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_50) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_50) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_50) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_50) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_50) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_50) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_50) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_50) * (int64_t)32) + (((int64_t)ax2_0_3_50) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_108 = 0; ax0_ax1_fused_0_108 < (int64_t)4; ++ax0_ax1_fused_0_108) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_108 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_108 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1728))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_109 = 0; ax0_ax1_fused_0_109 < (int64_t)4; ++ax0_ax1_fused_0_109) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_109 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_109 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1728))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_51 = 0; ax3_0_1_51 < (int64_t)2; ++ax3_0_1_51) {
    for (int64_t ax0_0_102 = 0; ax0_0_102 < (int64_t)4; ++ax0_0_102) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_102 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_51 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_102 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_102 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_102 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_102 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_103 = 0; ax0_0_103 < (int64_t)4; ++ax0_0_103) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_103 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_51 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_103 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_103 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_103 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_103 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_51 = 0; ax1_0_3_51 < 4; ++ax1_0_3_51) {
      for (int ax2_0_3_51 = 0; ax2_0_3_51 < 4; ++ax2_0_3_51) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_51) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_51) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_51) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_51) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_51) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_51) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_51) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_51) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_51) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_51) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_51) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_51) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_51) * (int64_t)32) + (((int64_t)ax2_0_3_51) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_110 = 0; ax0_ax1_fused_0_110 < (int64_t)4; ++ax0_ax1_fused_0_110) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_110 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_110 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1760))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_111 = 0; ax0_ax1_fused_0_111 < (int64_t)4; ++ax0_ax1_fused_0_111) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_111 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_111 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1760))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_52 = 0; ax3_0_1_52 < (int64_t)2; ++ax3_0_1_52) {
    for (int64_t ax0_0_104 = 0; ax0_0_104 < (int64_t)4; ++ax0_0_104) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_104 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_52 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_104 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_104 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_104 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_104 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_105 = 0; ax0_0_105 < (int64_t)4; ++ax0_0_105) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_105 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_52 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_105 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_105 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_105 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_105 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_52 = 0; ax1_0_3_52 < 4; ++ax1_0_3_52) {
      for (int ax2_0_3_52 = 0; ax2_0_3_52 < 4; ++ax2_0_3_52) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_52) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_52) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_52) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_52) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_52) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_52) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_52) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_52) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_52) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_52) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_52) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_52) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_52) * (int64_t)32) + (((int64_t)ax2_0_3_52) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_112 = 0; ax0_ax1_fused_0_112 < (int64_t)4; ++ax0_ax1_fused_0_112) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_112 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_112 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1792))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_113 = 0; ax0_ax1_fused_0_113 < (int64_t)4; ++ax0_ax1_fused_0_113) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_113 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_113 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1792))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_53 = 0; ax3_0_1_53 < (int64_t)2; ++ax3_0_1_53) {
    for (int64_t ax0_0_106 = 0; ax0_0_106 < (int64_t)4; ++ax0_0_106) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_106 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_53 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_106 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_106 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_106 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_106 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_107 = 0; ax0_0_107 < (int64_t)4; ++ax0_0_107) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_107 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_53 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_107 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_107 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_107 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_107 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_53 = 0; ax1_0_3_53 < 4; ++ax1_0_3_53) {
      for (int ax2_0_3_53 = 0; ax2_0_3_53 < 4; ++ax2_0_3_53) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_53) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_53) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_53) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_53) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_53) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_53) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_53) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_53) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_53) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_53) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_53) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_53) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_53) * (int64_t)32) + (((int64_t)ax2_0_3_53) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_114 = 0; ax0_ax1_fused_0_114 < (int64_t)4; ++ax0_ax1_fused_0_114) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_114 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_114 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1824))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_115 = 0; ax0_ax1_fused_0_115 < (int64_t)4; ++ax0_ax1_fused_0_115) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_115 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_115 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1824))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_54 = 0; ax3_0_1_54 < (int64_t)2; ++ax3_0_1_54) {
    for (int64_t ax0_0_108 = 0; ax0_0_108 < (int64_t)4; ++ax0_0_108) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_108 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_54 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_108 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_108 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_108 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_108 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_109 = 0; ax0_0_109 < (int64_t)4; ++ax0_0_109) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_109 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_54 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_109 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_109 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_109 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_109 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_54 = 0; ax1_0_3_54 < 4; ++ax1_0_3_54) {
      for (int ax2_0_3_54 = 0; ax2_0_3_54 < 4; ++ax2_0_3_54) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_54) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_54) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_54) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_54) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_54) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_54) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_54) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_54) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_54) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_54) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_54) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_54) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_54) * (int64_t)32) + (((int64_t)ax2_0_3_54) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_116 = 0; ax0_ax1_fused_0_116 < (int64_t)4; ++ax0_ax1_fused_0_116) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_116 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_116 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1856))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_117 = 0; ax0_ax1_fused_0_117 < (int64_t)4; ++ax0_ax1_fused_0_117) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_117 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_117 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1856))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_55 = 0; ax3_0_1_55 < (int64_t)2; ++ax3_0_1_55) {
    for (int64_t ax0_0_110 = 0; ax0_0_110 < (int64_t)4; ++ax0_0_110) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_110 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_55 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_110 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_110 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_110 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_110 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_111 = 0; ax0_0_111 < (int64_t)4; ++ax0_0_111) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_111 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_55 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_111 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_111 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_111 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_111 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_55 = 0; ax1_0_3_55 < 4; ++ax1_0_3_55) {
      for (int ax2_0_3_55 = 0; ax2_0_3_55 < 4; ++ax2_0_3_55) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_55) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_55) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_55) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_55) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_55) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_55) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_55) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_55) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_55) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_55) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_55) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_55) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_55) * (int64_t)32) + (((int64_t)ax2_0_3_55) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_118 = 0; ax0_ax1_fused_0_118 < (int64_t)4; ++ax0_ax1_fused_0_118) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_118 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_118 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1888))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_119 = 0; ax0_ax1_fused_0_119 < (int64_t)4; ++ax0_ax1_fused_0_119) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_119 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_119 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1888))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_56 = 0; ax3_0_1_56 < (int64_t)2; ++ax3_0_1_56) {
    for (int64_t ax0_0_112 = 0; ax0_0_112 < (int64_t)4; ++ax0_0_112) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_112 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_56 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_112 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_112 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_112 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_112 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_113 = 0; ax0_0_113 < (int64_t)4; ++ax0_0_113) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_113 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_56 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_113 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_113 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_113 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_113 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_56 = 0; ax1_0_3_56 < 4; ++ax1_0_3_56) {
      for (int ax2_0_3_56 = 0; ax2_0_3_56 < 4; ++ax2_0_3_56) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_56) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_56) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_56) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_56) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_56) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_56) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_56) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_56) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_56) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_56) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_56) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_56) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_56) * (int64_t)32) + (((int64_t)ax2_0_3_56) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_120 = 0; ax0_ax1_fused_0_120 < (int64_t)4; ++ax0_ax1_fused_0_120) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_120 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_120 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1920))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_121 = 0; ax0_ax1_fused_0_121 < (int64_t)4; ++ax0_ax1_fused_0_121) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_121 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_121 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1920))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_57 = 0; ax3_0_1_57 < (int64_t)2; ++ax3_0_1_57) {
    for (int64_t ax0_0_114 = 0; ax0_0_114 < (int64_t)4; ++ax0_0_114) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_114 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_57 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_114 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_114 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_114 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_114 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_115 = 0; ax0_0_115 < (int64_t)4; ++ax0_0_115) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_115 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_57 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_115 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_115 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_115 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_115 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_57 = 0; ax1_0_3_57 < 4; ++ax1_0_3_57) {
      for (int ax2_0_3_57 = 0; ax2_0_3_57 < 4; ++ax2_0_3_57) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_57) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_57) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_57) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_57) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_57) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_57) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_57) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_57) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_57) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_57) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_57) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_57) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_57) * (int64_t)32) + (((int64_t)ax2_0_3_57) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_122 = 0; ax0_ax1_fused_0_122 < (int64_t)4; ++ax0_ax1_fused_0_122) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_122 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_122 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1952))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_123 = 0; ax0_ax1_fused_0_123 < (int64_t)4; ++ax0_ax1_fused_0_123) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_123 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_123 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1952))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_58 = 0; ax3_0_1_58 < (int64_t)2; ++ax3_0_1_58) {
    for (int64_t ax0_0_116 = 0; ax0_0_116 < (int64_t)4; ++ax0_0_116) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_116 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_58 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_116 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_116 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_116 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_116 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_117 = 0; ax0_0_117 < (int64_t)4; ++ax0_0_117) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_117 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_58 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_117 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_117 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_117 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_117 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_58 = 0; ax1_0_3_58 < 4; ++ax1_0_3_58) {
      for (int ax2_0_3_58 = 0; ax2_0_3_58 < 4; ++ax2_0_3_58) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_58) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_58) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_58) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_58) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_58) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_58) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_58) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_58) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_58) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_58) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_58) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_58) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_58) * (int64_t)32) + (((int64_t)ax2_0_3_58) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_124 = 0; ax0_ax1_fused_0_124 < (int64_t)4; ++ax0_ax1_fused_0_124) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_124 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_124 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1984))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_125 = 0; ax0_ax1_fused_0_125 < (int64_t)4; ++ax0_ax1_fused_0_125) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_125 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_125 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)1984))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_59 = 0; ax3_0_1_59 < (int64_t)2; ++ax3_0_1_59) {
    for (int64_t ax0_0_118 = 0; ax0_0_118 < (int64_t)4; ++ax0_0_118) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_118 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_59 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_118 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_118 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_118 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_118 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_119 = 0; ax0_0_119 < (int64_t)4; ++ax0_0_119) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_119 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_59 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_119 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_119 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_119 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_119 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_59 = 0; ax1_0_3_59 < 4; ++ax1_0_3_59) {
      for (int ax2_0_3_59 = 0; ax2_0_3_59 < 4; ++ax2_0_3_59) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_59) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_59) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_59) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_59) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_59) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_59) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_59) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_59) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_59) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_59) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_59) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_59) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_59) * (int64_t)32) + (((int64_t)ax2_0_3_59) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_126 = 0; ax0_ax1_fused_0_126 < (int64_t)4; ++ax0_ax1_fused_0_126) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_126 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_126 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2016))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_127 = 0; ax0_ax1_fused_0_127 < (int64_t)4; ++ax0_ax1_fused_0_127) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_127 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_127 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2016))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_60 = 0; ax3_0_1_60 < (int64_t)2; ++ax3_0_1_60) {
    for (int64_t ax0_0_120 = 0; ax0_0_120 < (int64_t)4; ++ax0_0_120) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_120 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_60 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_120 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_120 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_120 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_120 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_121 = 0; ax0_0_121 < (int64_t)4; ++ax0_0_121) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_121 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_60 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_121 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_121 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_121 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_121 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_60 = 0; ax1_0_3_60 < 4; ++ax1_0_3_60) {
      for (int ax2_0_3_60 = 0; ax2_0_3_60 < 4; ++ax2_0_3_60) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_60) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_60) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_60) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_60) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_60) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_60) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_60) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_60) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_60) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_60) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_60) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_60) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_60) * (int64_t)32) + (((int64_t)ax2_0_3_60) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_128 = 0; ax0_ax1_fused_0_128 < (int64_t)4; ++ax0_ax1_fused_0_128) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_128 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_128 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2048))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_129 = 0; ax0_ax1_fused_0_129 < (int64_t)4; ++ax0_ax1_fused_0_129) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_129 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_129 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2048))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_61 = 0; ax3_0_1_61 < (int64_t)2; ++ax3_0_1_61) {
    for (int64_t ax0_0_122 = 0; ax0_0_122 < (int64_t)4; ++ax0_0_122) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_122 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_61 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_122 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_122 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_122 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_122 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_123 = 0; ax0_0_123 < (int64_t)4; ++ax0_0_123) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_123 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_61 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_123 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_123 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_123 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_123 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_61 = 0; ax1_0_3_61 < 4; ++ax1_0_3_61) {
      for (int ax2_0_3_61 = 0; ax2_0_3_61 < 4; ++ax2_0_3_61) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_61) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_61) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_61) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_61) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_61) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_61) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_61) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_61) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_61) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_61) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_61) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_61) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_61) * (int64_t)32) + (((int64_t)ax2_0_3_61) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_130 = 0; ax0_ax1_fused_0_130 < (int64_t)4; ++ax0_ax1_fused_0_130) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_130 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_130 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2080))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_131 = 0; ax0_ax1_fused_0_131 < (int64_t)4; ++ax0_ax1_fused_0_131) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_131 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_131 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2080))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_62 = 0; ax3_0_1_62 < (int64_t)2; ++ax3_0_1_62) {
    for (int64_t ax0_0_124 = 0; ax0_0_124 < (int64_t)4; ++ax0_0_124) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_124 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_62 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_124 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_124 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_124 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_124 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_125 = 0; ax0_0_125 < (int64_t)4; ++ax0_0_125) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_125 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_62 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_125 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_125 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_125 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_125 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_62 = 0; ax1_0_3_62 < 4; ++ax1_0_3_62) {
      for (int ax2_0_3_62 = 0; ax2_0_3_62 < 4; ++ax2_0_3_62) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_62) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_62) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_62) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_62) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_62) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_62) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_62) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_62) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_62) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_62) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_62) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_62) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_62) * (int64_t)32) + (((int64_t)ax2_0_3_62) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_132 = 0; ax0_ax1_fused_0_132 < (int64_t)4; ++ax0_ax1_fused_0_132) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_132 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_132 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2112))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_133 = 0; ax0_ax1_fused_0_133 < (int64_t)4; ++ax0_ax1_fused_0_133) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_133 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_133 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2112))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_63 = 0; ax3_0_1_63 < (int64_t)2; ++ax3_0_1_63) {
    for (int64_t ax0_0_126 = 0; ax0_0_126 < (int64_t)4; ++ax0_0_126) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_126 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_63 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_126 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_126 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_126 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_126 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_127 = 0; ax0_0_127 < (int64_t)4; ++ax0_0_127) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_127 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_63 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_127 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_127 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_127 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_127 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_63 = 0; ax1_0_3_63 < 4; ++ax1_0_3_63) {
      for (int ax2_0_3_63 = 0; ax2_0_3_63 < 4; ++ax2_0_3_63) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_63) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_63) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_63) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_63) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_63) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_63) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_63) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_63) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_63) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_63) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_63) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_63) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_63) * (int64_t)32) + (((int64_t)ax2_0_3_63) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_134 = 0; ax0_ax1_fused_0_134 < (int64_t)4; ++ax0_ax1_fused_0_134) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_134 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_134 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2144))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_135 = 0; ax0_ax1_fused_0_135 < (int64_t)4; ++ax0_ax1_fused_0_135) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_135 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_135 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2144))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_64 = 0; ax3_0_1_64 < (int64_t)2; ++ax3_0_1_64) {
    for (int64_t ax0_0_128 = 0; ax0_0_128 < (int64_t)4; ++ax0_0_128) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_128 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_64 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_128 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_128 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_128 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_128 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_129 = 0; ax0_0_129 < (int64_t)4; ++ax0_0_129) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_129 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_64 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_129 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_129 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_129 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_129 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_64 = 0; ax1_0_3_64 < 4; ++ax1_0_3_64) {
      for (int ax2_0_3_64 = 0; ax2_0_3_64 < 4; ++ax2_0_3_64) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_64) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_64) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_64) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_64) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_64) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_64) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_64) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_64) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_64) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_64) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_64) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_64) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_64) * (int64_t)32) + (((int64_t)ax2_0_3_64) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_136 = 0; ax0_ax1_fused_0_136 < (int64_t)4; ++ax0_ax1_fused_0_136) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_136 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_136 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2176))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_137 = 0; ax0_ax1_fused_0_137 < (int64_t)4; ++ax0_ax1_fused_0_137) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_137 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_137 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2176))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_65 = 0; ax3_0_1_65 < (int64_t)2; ++ax3_0_1_65) {
    for (int64_t ax0_0_130 = 0; ax0_0_130 < (int64_t)4; ++ax0_0_130) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_130 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_65 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_130 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_130 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_130 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_130 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_131 = 0; ax0_0_131 < (int64_t)4; ++ax0_0_131) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_131 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_65 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_131 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_131 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_131 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_131 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_65 = 0; ax1_0_3_65 < 4; ++ax1_0_3_65) {
      for (int ax2_0_3_65 = 0; ax2_0_3_65 < 4; ++ax2_0_3_65) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_65) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_65) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_65) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_65) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_65) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_65) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_65) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_65) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_65) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_65) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_65) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_65) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_65) * (int64_t)32) + (((int64_t)ax2_0_3_65) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_138 = 0; ax0_ax1_fused_0_138 < (int64_t)4; ++ax0_ax1_fused_0_138) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_138 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_138 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2208))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_139 = 0; ax0_ax1_fused_0_139 < (int64_t)4; ++ax0_ax1_fused_0_139) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_139 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_139 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2208))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_66 = 0; ax3_0_1_66 < (int64_t)2; ++ax3_0_1_66) {
    for (int64_t ax0_0_132 = 0; ax0_0_132 < (int64_t)4; ++ax0_0_132) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_132 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_66 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_132 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_132 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_132 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_132 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_133 = 0; ax0_0_133 < (int64_t)4; ++ax0_0_133) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_133 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_66 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_133 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_133 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_133 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_133 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_66 = 0; ax1_0_3_66 < 4; ++ax1_0_3_66) {
      for (int ax2_0_3_66 = 0; ax2_0_3_66 < 4; ++ax2_0_3_66) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_66) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_66) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_66) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_66) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_66) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_66) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_66) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_66) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_66) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_66) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_66) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_66) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_66) * (int64_t)32) + (((int64_t)ax2_0_3_66) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_140 = 0; ax0_ax1_fused_0_140 < (int64_t)4; ++ax0_ax1_fused_0_140) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_140 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_140 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2240))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_141 = 0; ax0_ax1_fused_0_141 < (int64_t)4; ++ax0_ax1_fused_0_141) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_141 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_141 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2240))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_67 = 0; ax3_0_1_67 < (int64_t)2; ++ax3_0_1_67) {
    for (int64_t ax0_0_134 = 0; ax0_0_134 < (int64_t)4; ++ax0_0_134) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_134 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_67 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_134 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_134 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_134 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_134 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_135 = 0; ax0_0_135 < (int64_t)4; ++ax0_0_135) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_135 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_67 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_135 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_135 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_135 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_135 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_67 = 0; ax1_0_3_67 < 4; ++ax1_0_3_67) {
      for (int ax2_0_3_67 = 0; ax2_0_3_67 < 4; ++ax2_0_3_67) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_67) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_67) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_67) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_67) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_67) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_67) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_67) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_67) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_67) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_67) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_67) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_67) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_67) * (int64_t)32) + (((int64_t)ax2_0_3_67) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_142 = 0; ax0_ax1_fused_0_142 < (int64_t)4; ++ax0_ax1_fused_0_142) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_142 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_142 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2272))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_143 = 0; ax0_ax1_fused_0_143 < (int64_t)4; ++ax0_ax1_fused_0_143) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_143 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_143 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2272))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_68 = 0; ax3_0_1_68 < (int64_t)2; ++ax3_0_1_68) {
    for (int64_t ax0_0_136 = 0; ax0_0_136 < (int64_t)4; ++ax0_0_136) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_136 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_68 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_136 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_136 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_136 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_136 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_137 = 0; ax0_0_137 < (int64_t)4; ++ax0_0_137) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_137 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_68 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_137 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_137 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_137 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_137 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_68 = 0; ax1_0_3_68 < 4; ++ax1_0_3_68) {
      for (int ax2_0_3_68 = 0; ax2_0_3_68 < 4; ++ax2_0_3_68) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_68) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_68) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_68) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_68) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_68) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_68) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_68) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_68) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_68) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_68) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_68) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_68) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_68) * (int64_t)32) + (((int64_t)ax2_0_3_68) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_144 = 0; ax0_ax1_fused_0_144 < (int64_t)4; ++ax0_ax1_fused_0_144) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_144 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_144 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2304))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_145 = 0; ax0_ax1_fused_0_145 < (int64_t)4; ++ax0_ax1_fused_0_145) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_145 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_145 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2304))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_69 = 0; ax3_0_1_69 < (int64_t)2; ++ax3_0_1_69) {
    for (int64_t ax0_0_138 = 0; ax0_0_138 < (int64_t)4; ++ax0_0_138) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_138 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_69 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_138 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_138 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_138 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_138 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_139 = 0; ax0_0_139 < (int64_t)4; ++ax0_0_139) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_139 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_69 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_139 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_139 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_139 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_139 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_69 = 0; ax1_0_3_69 < 4; ++ax1_0_3_69) {
      for (int ax2_0_3_69 = 0; ax2_0_3_69 < 4; ++ax2_0_3_69) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_69) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_69) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_69) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_69) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_69) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_69) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_69) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_69) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_69) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_69) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_69) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_69) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_69) * (int64_t)32) + (((int64_t)ax2_0_3_69) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_146 = 0; ax0_ax1_fused_0_146 < (int64_t)4; ++ax0_ax1_fused_0_146) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_146 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_146 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2336))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_147 = 0; ax0_ax1_fused_0_147 < (int64_t)4; ++ax0_ax1_fused_0_147) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_147 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_147 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2336))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_70 = 0; ax3_0_1_70 < (int64_t)2; ++ax3_0_1_70) {
    for (int64_t ax0_0_140 = 0; ax0_0_140 < (int64_t)4; ++ax0_0_140) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_140 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_70 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_140 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_140 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_140 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_140 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_141 = 0; ax0_0_141 < (int64_t)4; ++ax0_0_141) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_141 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_70 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_141 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_141 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_141 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_141 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_70 = 0; ax1_0_3_70 < 4; ++ax1_0_3_70) {
      for (int ax2_0_3_70 = 0; ax2_0_3_70 < 4; ++ax2_0_3_70) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_70) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_70) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_70) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_70) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_70) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_70) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_70) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_70) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_70) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_70) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_70) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_70) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_70) * (int64_t)32) + (((int64_t)ax2_0_3_70) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_148 = 0; ax0_ax1_fused_0_148 < (int64_t)4; ++ax0_ax1_fused_0_148) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_148 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_148 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2368))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_149 = 0; ax0_ax1_fused_0_149 < (int64_t)4; ++ax0_ax1_fused_0_149) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_149 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_149 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2368))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_71 = 0; ax3_0_1_71 < (int64_t)2; ++ax3_0_1_71) {
    for (int64_t ax0_0_142 = 0; ax0_0_142 < (int64_t)4; ++ax0_0_142) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_142 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_71 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_142 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_142 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_142 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_142 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_143 = 0; ax0_0_143 < (int64_t)4; ++ax0_0_143) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_143 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_71 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_143 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_143 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_143 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_143 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_71 = 0; ax1_0_3_71 < 4; ++ax1_0_3_71) {
      for (int ax2_0_3_71 = 0; ax2_0_3_71 < 4; ++ax2_0_3_71) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_71) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_71) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_71) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_71) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_71) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_71) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_71) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_71) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_71) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_71) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_71) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_71) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_71) * (int64_t)32) + (((int64_t)ax2_0_3_71) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_150 = 0; ax0_ax1_fused_0_150 < (int64_t)4; ++ax0_ax1_fused_0_150) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_150 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_150 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2400))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_151 = 0; ax0_ax1_fused_0_151 < (int64_t)4; ++ax0_ax1_fused_0_151) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_151 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_151 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2400))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_72 = 0; ax3_0_1_72 < (int64_t)2; ++ax3_0_1_72) {
    for (int64_t ax0_0_144 = 0; ax0_0_144 < (int64_t)4; ++ax0_0_144) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_144 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_72 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_144 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_144 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_144 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_144 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_145 = 0; ax0_0_145 < (int64_t)4; ++ax0_0_145) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_145 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_72 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_145 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_145 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_145 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_145 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_72 = 0; ax1_0_3_72 < 4; ++ax1_0_3_72) {
      for (int ax2_0_3_72 = 0; ax2_0_3_72 < 4; ++ax2_0_3_72) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_72) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_72) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_72) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_72) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_72) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_72) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_72) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_72) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_72) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_72) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_72) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_72) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_72) * (int64_t)32) + (((int64_t)ax2_0_3_72) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_152 = 0; ax0_ax1_fused_0_152 < (int64_t)4; ++ax0_ax1_fused_0_152) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_152 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_152 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2432))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_153 = 0; ax0_ax1_fused_0_153 < (int64_t)4; ++ax0_ax1_fused_0_153) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_153 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_153 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2432))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_73 = 0; ax3_0_1_73 < (int64_t)2; ++ax3_0_1_73) {
    for (int64_t ax0_0_146 = 0; ax0_0_146 < (int64_t)4; ++ax0_0_146) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_146 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_73 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_146 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_146 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_146 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_146 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_147 = 0; ax0_0_147 < (int64_t)4; ++ax0_0_147) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_147 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_73 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_147 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_147 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_147 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_147 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_73 = 0; ax1_0_3_73 < 4; ++ax1_0_3_73) {
      for (int ax2_0_3_73 = 0; ax2_0_3_73 < 4; ++ax2_0_3_73) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_73) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_73) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_73) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_73) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_73) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_73) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_73) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_73) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_73) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_73) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_73) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_73) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_73) * (int64_t)32) + (((int64_t)ax2_0_3_73) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_154 = 0; ax0_ax1_fused_0_154 < (int64_t)4; ++ax0_ax1_fused_0_154) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_154 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_154 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2464))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_155 = 0; ax0_ax1_fused_0_155 < (int64_t)4; ++ax0_ax1_fused_0_155) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_155 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_155 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2464))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_74 = 0; ax3_0_1_74 < (int64_t)2; ++ax3_0_1_74) {
    for (int64_t ax0_0_148 = 0; ax0_0_148 < (int64_t)4; ++ax0_0_148) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_148 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_74 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_148 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_148 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_148 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_148 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_149 = 0; ax0_0_149 < (int64_t)4; ++ax0_0_149) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_149 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_74 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_149 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_149 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_149 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_149 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_74 = 0; ax1_0_3_74 < 4; ++ax1_0_3_74) {
      for (int ax2_0_3_74 = 0; ax2_0_3_74 < 4; ++ax2_0_3_74) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_74) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_74) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_74) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_74) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_74) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_74) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_74) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_74) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_74) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_74) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_74) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_74) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_74) * (int64_t)32) + (((int64_t)ax2_0_3_74) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_156 = 0; ax0_ax1_fused_0_156 < (int64_t)4; ++ax0_ax1_fused_0_156) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_156 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_156 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2496))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_157 = 0; ax0_ax1_fused_0_157 < (int64_t)4; ++ax0_ax1_fused_0_157) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_157 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_157 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2496))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_75 = 0; ax3_0_1_75 < (int64_t)2; ++ax3_0_1_75) {
    for (int64_t ax0_0_150 = 0; ax0_0_150 < (int64_t)4; ++ax0_0_150) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_150 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_75 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_150 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_150 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_150 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_150 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_151 = 0; ax0_0_151 < (int64_t)4; ++ax0_0_151) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_151 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_75 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_151 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_151 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_151 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_151 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_75 = 0; ax1_0_3_75 < 4; ++ax1_0_3_75) {
      for (int ax2_0_3_75 = 0; ax2_0_3_75 < 4; ++ax2_0_3_75) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_75) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_75) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_75) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_75) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_75) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_75) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_75) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_75) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_75) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_75) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_75) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_75) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_75) * (int64_t)32) + (((int64_t)ax2_0_3_75) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_158 = 0; ax0_ax1_fused_0_158 < (int64_t)4; ++ax0_ax1_fused_0_158) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_158 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_158 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2528))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_159 = 0; ax0_ax1_fused_0_159 < (int64_t)4; ++ax0_ax1_fused_0_159) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_159 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_159 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2528))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_76 = 0; ax3_0_1_76 < (int64_t)2; ++ax3_0_1_76) {
    for (int64_t ax0_0_152 = 0; ax0_0_152 < (int64_t)4; ++ax0_0_152) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_152 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_76 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_152 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_152 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_152 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_152 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_153 = 0; ax0_0_153 < (int64_t)4; ++ax0_0_153) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_153 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_76 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_153 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_153 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_153 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_153 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_76 = 0; ax1_0_3_76 < 4; ++ax1_0_3_76) {
      for (int ax2_0_3_76 = 0; ax2_0_3_76 < 4; ++ax2_0_3_76) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_76) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_76) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_76) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_76) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_76) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_76) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_76) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_76) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_76) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_76) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_76) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_76) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_76) * (int64_t)32) + (((int64_t)ax2_0_3_76) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_160 = 0; ax0_ax1_fused_0_160 < (int64_t)4; ++ax0_ax1_fused_0_160) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_160 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_160 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2560))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_161 = 0; ax0_ax1_fused_0_161 < (int64_t)4; ++ax0_ax1_fused_0_161) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_161 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_161 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2560))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_77 = 0; ax3_0_1_77 < (int64_t)2; ++ax3_0_1_77) {
    for (int64_t ax0_0_154 = 0; ax0_0_154 < (int64_t)4; ++ax0_0_154) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_154 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_77 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_154 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_154 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_154 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_154 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_155 = 0; ax0_0_155 < (int64_t)4; ++ax0_0_155) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_155 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_77 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_155 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_155 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_155 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_155 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_77 = 0; ax1_0_3_77 < 4; ++ax1_0_3_77) {
      for (int ax2_0_3_77 = 0; ax2_0_3_77 < 4; ++ax2_0_3_77) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_77) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_77) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_77) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_77) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_77) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_77) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_77) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_77) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_77) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_77) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_77) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_77) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_77) * (int64_t)32) + (((int64_t)ax2_0_3_77) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_162 = 0; ax0_ax1_fused_0_162 < (int64_t)4; ++ax0_ax1_fused_0_162) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_162 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_162 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2592))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_163 = 0; ax0_ax1_fused_0_163 < (int64_t)4; ++ax0_ax1_fused_0_163) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_163 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_163 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2592))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_78 = 0; ax3_0_1_78 < (int64_t)2; ++ax3_0_1_78) {
    for (int64_t ax0_0_156 = 0; ax0_0_156 < (int64_t)4; ++ax0_0_156) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_156 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_78 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_156 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_156 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_156 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_156 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_157 = 0; ax0_0_157 < (int64_t)4; ++ax0_0_157) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_157 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_78 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_157 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_157 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_157 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_157 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_78 = 0; ax1_0_3_78 < 4; ++ax1_0_3_78) {
      for (int ax2_0_3_78 = 0; ax2_0_3_78 < 4; ++ax2_0_3_78) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_78) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_78) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_78) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_78) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_78) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_78) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_78) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_78) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_78) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_78) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_78) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_78) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_78) * (int64_t)32) + (((int64_t)ax2_0_3_78) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_164 = 0; ax0_ax1_fused_0_164 < (int64_t)4; ++ax0_ax1_fused_0_164) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_164 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_164 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2624))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_165 = 0; ax0_ax1_fused_0_165 < (int64_t)4; ++ax0_ax1_fused_0_165) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_165 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_165 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2624))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_79 = 0; ax3_0_1_79 < (int64_t)2; ++ax3_0_1_79) {
    for (int64_t ax0_0_158 = 0; ax0_0_158 < (int64_t)4; ++ax0_0_158) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_158 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_79 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_158 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_158 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_158 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_158 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_159 = 0; ax0_0_159 < (int64_t)4; ++ax0_0_159) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_159 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_79 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_159 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_159 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_159 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_159 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_79 = 0; ax1_0_3_79 < 4; ++ax1_0_3_79) {
      for (int ax2_0_3_79 = 0; ax2_0_3_79 < 4; ++ax2_0_3_79) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_79) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_79) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_79) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_79) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_79) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_79) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_79) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_79) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_79) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_79) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_79) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_79) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_79) * (int64_t)32) + (((int64_t)ax2_0_3_79) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_166 = 0; ax0_ax1_fused_0_166 < (int64_t)4; ++ax0_ax1_fused_0_166) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_166 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_166 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2656))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_167 = 0; ax0_ax1_fused_0_167 < (int64_t)4; ++ax0_ax1_fused_0_167) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_167 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_167 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2656))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_80 = 0; ax3_0_1_80 < (int64_t)2; ++ax3_0_1_80) {
    for (int64_t ax0_0_160 = 0; ax0_0_160 < (int64_t)4; ++ax0_0_160) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_160 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_80 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_160 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_160 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_160 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_160 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_161 = 0; ax0_0_161 < (int64_t)4; ++ax0_0_161) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_161 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_80 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_161 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_161 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_161 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_161 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_80 = 0; ax1_0_3_80 < 4; ++ax1_0_3_80) {
      for (int ax2_0_3_80 = 0; ax2_0_3_80 < 4; ++ax2_0_3_80) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_80) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_80) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_80) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_80) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_80) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_80) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_80) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_80) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_80) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_80) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_80) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_80) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_80) * (int64_t)32) + (((int64_t)ax2_0_3_80) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_168 = 0; ax0_ax1_fused_0_168 < (int64_t)4; ++ax0_ax1_fused_0_168) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_168 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_168 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2688))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_169 = 0; ax0_ax1_fused_0_169 < (int64_t)4; ++ax0_ax1_fused_0_169) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_169 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_169 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2688))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_81 = 0; ax3_0_1_81 < (int64_t)2; ++ax3_0_1_81) {
    for (int64_t ax0_0_162 = 0; ax0_0_162 < (int64_t)4; ++ax0_0_162) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_162 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_81 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_162 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_162 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_162 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_162 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_163 = 0; ax0_0_163 < (int64_t)4; ++ax0_0_163) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_163 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_81 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_163 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_163 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_163 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_163 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_81 = 0; ax1_0_3_81 < 4; ++ax1_0_3_81) {
      for (int ax2_0_3_81 = 0; ax2_0_3_81 < 4; ++ax2_0_3_81) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_81) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_81) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_81) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_81) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_81) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_81) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_81) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_81) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_81) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_81) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_81) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_81) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_81) * (int64_t)32) + (((int64_t)ax2_0_3_81) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_170 = 0; ax0_ax1_fused_0_170 < (int64_t)4; ++ax0_ax1_fused_0_170) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_170 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_170 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2720))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_171 = 0; ax0_ax1_fused_0_171 < (int64_t)4; ++ax0_ax1_fused_0_171) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_171 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_171 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2720))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_82 = 0; ax3_0_1_82 < (int64_t)2; ++ax3_0_1_82) {
    for (int64_t ax0_0_164 = 0; ax0_0_164 < (int64_t)4; ++ax0_0_164) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_164 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_82 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_164 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_164 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_164 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_164 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_165 = 0; ax0_0_165 < (int64_t)4; ++ax0_0_165) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_165 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_82 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_165 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_165 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_165 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_165 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_82 = 0; ax1_0_3_82 < 4; ++ax1_0_3_82) {
      for (int ax2_0_3_82 = 0; ax2_0_3_82 < 4; ++ax2_0_3_82) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_82) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_82) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_82) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_82) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_82) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_82) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_82) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_82) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_82) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_82) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_82) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_82) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_82) * (int64_t)32) + (((int64_t)ax2_0_3_82) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_172 = 0; ax0_ax1_fused_0_172 < (int64_t)4; ++ax0_ax1_fused_0_172) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_172 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_172 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2752))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_173 = 0; ax0_ax1_fused_0_173 < (int64_t)4; ++ax0_ax1_fused_0_173) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_173 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_173 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2752))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_83 = 0; ax3_0_1_83 < (int64_t)2; ++ax3_0_1_83) {
    for (int64_t ax0_0_166 = 0; ax0_0_166 < (int64_t)4; ++ax0_0_166) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_166 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_83 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_166 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_166 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_166 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_166 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_167 = 0; ax0_0_167 < (int64_t)4; ++ax0_0_167) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_167 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_83 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_167 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_167 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_167 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_167 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_83 = 0; ax1_0_3_83 < 4; ++ax1_0_3_83) {
      for (int ax2_0_3_83 = 0; ax2_0_3_83 < 4; ++ax2_0_3_83) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_83) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_83) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_83) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_83) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_83) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_83) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_83) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_83) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_83) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_83) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_83) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_83) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_83) * (int64_t)32) + (((int64_t)ax2_0_3_83) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_174 = 0; ax0_ax1_fused_0_174 < (int64_t)4; ++ax0_ax1_fused_0_174) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_174 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_174 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2784))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_175 = 0; ax0_ax1_fused_0_175 < (int64_t)4; ++ax0_ax1_fused_0_175) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_175 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_175 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2784))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_84 = 0; ax3_0_1_84 < (int64_t)2; ++ax3_0_1_84) {
    for (int64_t ax0_0_168 = 0; ax0_0_168 < (int64_t)4; ++ax0_0_168) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_168 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_84 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_168 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_168 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_168 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_168 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_169 = 0; ax0_0_169 < (int64_t)4; ++ax0_0_169) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_169 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_84 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_169 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_169 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_169 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_169 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_84 = 0; ax1_0_3_84 < 4; ++ax1_0_3_84) {
      for (int ax2_0_3_84 = 0; ax2_0_3_84 < 4; ++ax2_0_3_84) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_84) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_84) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_84) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_84) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_84) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_84) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_84) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_84) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_84) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_84) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_84) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_84) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_84) * (int64_t)32) + (((int64_t)ax2_0_3_84) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_176 = 0; ax0_ax1_fused_0_176 < (int64_t)4; ++ax0_ax1_fused_0_176) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_176 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_176 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2816))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_177 = 0; ax0_ax1_fused_0_177 < (int64_t)4; ++ax0_ax1_fused_0_177) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_177 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_177 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2816))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_85 = 0; ax3_0_1_85 < (int64_t)2; ++ax3_0_1_85) {
    for (int64_t ax0_0_170 = 0; ax0_0_170 < (int64_t)4; ++ax0_0_170) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_170 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_85 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_170 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_170 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_170 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_170 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_171 = 0; ax0_0_171 < (int64_t)4; ++ax0_0_171) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_171 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_85 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_171 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_171 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_171 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_171 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_85 = 0; ax1_0_3_85 < 4; ++ax1_0_3_85) {
      for (int ax2_0_3_85 = 0; ax2_0_3_85 < 4; ++ax2_0_3_85) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_85) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_85) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_85) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_85) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_85) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_85) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_85) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_85) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_85) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_85) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_85) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_85) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_85) * (int64_t)32) + (((int64_t)ax2_0_3_85) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_178 = 0; ax0_ax1_fused_0_178 < (int64_t)4; ++ax0_ax1_fused_0_178) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_178 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_178 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2848))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_179 = 0; ax0_ax1_fused_0_179 < (int64_t)4; ++ax0_ax1_fused_0_179) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_179 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_179 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2848))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_86 = 0; ax3_0_1_86 < (int64_t)2; ++ax3_0_1_86) {
    for (int64_t ax0_0_172 = 0; ax0_0_172 < (int64_t)4; ++ax0_0_172) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_172 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_86 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_172 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_172 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_172 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_172 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_173 = 0; ax0_0_173 < (int64_t)4; ++ax0_0_173) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_173 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_86 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_173 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_173 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_173 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_173 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_86 = 0; ax1_0_3_86 < 4; ++ax1_0_3_86) {
      for (int ax2_0_3_86 = 0; ax2_0_3_86 < 4; ++ax2_0_3_86) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_86) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_86) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_86) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_86) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_86) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_86) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_86) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_86) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_86) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_86) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_86) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_86) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_86) * (int64_t)32) + (((int64_t)ax2_0_3_86) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_180 = 0; ax0_ax1_fused_0_180 < (int64_t)4; ++ax0_ax1_fused_0_180) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_180 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_180 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2880))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_181 = 0; ax0_ax1_fused_0_181 < (int64_t)4; ++ax0_ax1_fused_0_181) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_181 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_181 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2880))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_87 = 0; ax3_0_1_87 < (int64_t)2; ++ax3_0_1_87) {
    for (int64_t ax0_0_174 = 0; ax0_0_174 < (int64_t)4; ++ax0_0_174) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_174 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_87 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_174 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_174 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_174 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_174 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_175 = 0; ax0_0_175 < (int64_t)4; ++ax0_0_175) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_175 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_87 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_175 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_175 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_175 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_175 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_87 = 0; ax1_0_3_87 < 4; ++ax1_0_3_87) {
      for (int ax2_0_3_87 = 0; ax2_0_3_87 < 4; ++ax2_0_3_87) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_87) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_87) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_87) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_87) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_87) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_87) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_87) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_87) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_87) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_87) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_87) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_87) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_87) * (int64_t)32) + (((int64_t)ax2_0_3_87) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_182 = 0; ax0_ax1_fused_0_182 < (int64_t)4; ++ax0_ax1_fused_0_182) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_182 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_182 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2912))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_183 = 0; ax0_ax1_fused_0_183 < (int64_t)4; ++ax0_ax1_fused_0_183) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_183 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_183 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2912))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_88 = 0; ax3_0_1_88 < (int64_t)2; ++ax3_0_1_88) {
    for (int64_t ax0_0_176 = 0; ax0_0_176 < (int64_t)4; ++ax0_0_176) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_176 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_88 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_176 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_176 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_176 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_176 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_177 = 0; ax0_0_177 < (int64_t)4; ++ax0_0_177) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_177 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_88 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_177 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_177 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_177 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_177 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_88 = 0; ax1_0_3_88 < 4; ++ax1_0_3_88) {
      for (int ax2_0_3_88 = 0; ax2_0_3_88 < 4; ++ax2_0_3_88) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_88) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_88) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_88) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_88) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_88) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_88) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_88) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_88) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_88) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_88) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_88) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_88) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_88) * (int64_t)32) + (((int64_t)ax2_0_3_88) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_184 = 0; ax0_ax1_fused_0_184 < (int64_t)4; ++ax0_ax1_fused_0_184) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_184 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_184 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2944))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_185 = 0; ax0_ax1_fused_0_185 < (int64_t)4; ++ax0_ax1_fused_0_185) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_185 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_185 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2944))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_89 = 0; ax3_0_1_89 < (int64_t)2; ++ax3_0_1_89) {
    for (int64_t ax0_0_178 = 0; ax0_0_178 < (int64_t)4; ++ax0_0_178) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_178 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_89 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_178 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_178 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_178 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_178 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_179 = 0; ax0_0_179 < (int64_t)4; ++ax0_0_179) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_179 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_89 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_179 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_179 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_179 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_179 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_89 = 0; ax1_0_3_89 < 4; ++ax1_0_3_89) {
      for (int ax2_0_3_89 = 0; ax2_0_3_89 < 4; ++ax2_0_3_89) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_89) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_89) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_89) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_89) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_89) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_89) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_89) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_89) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_89) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_89) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_89) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_89) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_89) * (int64_t)32) + (((int64_t)ax2_0_3_89) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_186 = 0; ax0_ax1_fused_0_186 < (int64_t)4; ++ax0_ax1_fused_0_186) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_186 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_186 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2976))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_187 = 0; ax0_ax1_fused_0_187 < (int64_t)4; ++ax0_ax1_fused_0_187) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_187 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_187 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)2976))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_90 = 0; ax3_0_1_90 < (int64_t)2; ++ax3_0_1_90) {
    for (int64_t ax0_0_180 = 0; ax0_0_180 < (int64_t)4; ++ax0_0_180) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_180 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_90 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_180 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_180 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_180 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_180 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_181 = 0; ax0_0_181 < (int64_t)4; ++ax0_0_181) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_181 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_90 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_181 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_181 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_181 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_181 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_90 = 0; ax1_0_3_90 < 4; ++ax1_0_3_90) {
      for (int ax2_0_3_90 = 0; ax2_0_3_90 < 4; ++ax2_0_3_90) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_90) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_90) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_90) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_90) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_90) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_90) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_90) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_90) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_90) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_90) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_90) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_90) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_90) * (int64_t)32) + (((int64_t)ax2_0_3_90) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_188 = 0; ax0_ax1_fused_0_188 < (int64_t)4; ++ax0_ax1_fused_0_188) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_188 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_188 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3008))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_189 = 0; ax0_ax1_fused_0_189 < (int64_t)4; ++ax0_ax1_fused_0_189) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_189 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_189 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3008))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_91 = 0; ax3_0_1_91 < (int64_t)2; ++ax3_0_1_91) {
    for (int64_t ax0_0_182 = 0; ax0_0_182 < (int64_t)4; ++ax0_0_182) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_182 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_91 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_182 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_182 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_182 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_182 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_183 = 0; ax0_0_183 < (int64_t)4; ++ax0_0_183) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_183 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_91 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_183 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_183 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_183 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_183 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_91 = 0; ax1_0_3_91 < 4; ++ax1_0_3_91) {
      for (int ax2_0_3_91 = 0; ax2_0_3_91 < 4; ++ax2_0_3_91) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_91) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_91) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_91) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_91) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_91) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_91) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_91) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_91) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_91) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_91) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_91) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_91) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_91) * (int64_t)32) + (((int64_t)ax2_0_3_91) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_190 = 0; ax0_ax1_fused_0_190 < (int64_t)4; ++ax0_ax1_fused_0_190) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_190 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_190 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3040))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_191 = 0; ax0_ax1_fused_0_191 < (int64_t)4; ++ax0_ax1_fused_0_191) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_191 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_191 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3040))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_92 = 0; ax3_0_1_92 < (int64_t)2; ++ax3_0_1_92) {
    for (int64_t ax0_0_184 = 0; ax0_0_184 < (int64_t)4; ++ax0_0_184) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_184 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_92 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_184 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_184 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_184 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_184 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_185 = 0; ax0_0_185 < (int64_t)4; ++ax0_0_185) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_185 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_92 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_185 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_185 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_185 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_185 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_92 = 0; ax1_0_3_92 < 4; ++ax1_0_3_92) {
      for (int ax2_0_3_92 = 0; ax2_0_3_92 < 4; ++ax2_0_3_92) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_92) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_92) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_92) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_92) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_92) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_92) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_92) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_92) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_92) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_92) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_92) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_92) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_92) * (int64_t)32) + (((int64_t)ax2_0_3_92) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_192 = 0; ax0_ax1_fused_0_192 < (int64_t)4; ++ax0_ax1_fused_0_192) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_192 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_192 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3072))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_193 = 0; ax0_ax1_fused_0_193 < (int64_t)4; ++ax0_ax1_fused_0_193) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_193 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_193 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3072))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_93 = 0; ax3_0_1_93 < (int64_t)2; ++ax3_0_1_93) {
    for (int64_t ax0_0_186 = 0; ax0_0_186 < (int64_t)4; ++ax0_0_186) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_186 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_93 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_186 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_186 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_186 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_186 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_187 = 0; ax0_0_187 < (int64_t)4; ++ax0_0_187) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_187 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_93 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_187 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_187 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_187 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_187 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_93 = 0; ax1_0_3_93 < 4; ++ax1_0_3_93) {
      for (int ax2_0_3_93 = 0; ax2_0_3_93 < 4; ++ax2_0_3_93) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_93) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_93) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_93) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_93) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_93) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_93) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_93) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_93) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_93) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_93) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_93) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_93) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_93) * (int64_t)32) + (((int64_t)ax2_0_3_93) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_194 = 0; ax0_ax1_fused_0_194 < (int64_t)4; ++ax0_ax1_fused_0_194) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_194 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_194 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3104))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_195 = 0; ax0_ax1_fused_0_195 < (int64_t)4; ++ax0_ax1_fused_0_195) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_195 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_195 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3104))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_94 = 0; ax3_0_1_94 < (int64_t)2; ++ax3_0_1_94) {
    for (int64_t ax0_0_188 = 0; ax0_0_188 < (int64_t)4; ++ax0_0_188) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_188 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_94 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_188 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_188 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_188 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_188 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_189 = 0; ax0_0_189 < (int64_t)4; ++ax0_0_189) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_189 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_94 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_189 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_189 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_189 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_189 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_94 = 0; ax1_0_3_94 < 4; ++ax1_0_3_94) {
      for (int ax2_0_3_94 = 0; ax2_0_3_94 < 4; ++ax2_0_3_94) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_94) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_94) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_94) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_94) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_94) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_94) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_94) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_94) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_94) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_94) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_94) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_94) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_94) * (int64_t)32) + (((int64_t)ax2_0_3_94) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_196 = 0; ax0_ax1_fused_0_196 < (int64_t)4; ++ax0_ax1_fused_0_196) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_196 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_196 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3136))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_197 = 0; ax0_ax1_fused_0_197 < (int64_t)4; ++ax0_ax1_fused_0_197) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_197 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_197 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3136))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_95 = 0; ax3_0_1_95 < (int64_t)2; ++ax3_0_1_95) {
    for (int64_t ax0_0_190 = 0; ax0_0_190 < (int64_t)4; ++ax0_0_190) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_190 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_95 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_190 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_190 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_190 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_190 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_191 = 0; ax0_0_191 < (int64_t)4; ++ax0_0_191) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_191 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_95 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_191 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_191 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_191 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_191 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_95 = 0; ax1_0_3_95 < 4; ++ax1_0_3_95) {
      for (int ax2_0_3_95 = 0; ax2_0_3_95 < 4; ++ax2_0_3_95) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_95) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_95) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_95) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_95) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_95) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_95) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_95) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_95) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_95) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_95) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_95) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_95) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_95) * (int64_t)32) + (((int64_t)ax2_0_3_95) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_198 = 0; ax0_ax1_fused_0_198 < (int64_t)4; ++ax0_ax1_fused_0_198) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_198 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_198 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3168))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_199 = 0; ax0_ax1_fused_0_199 < (int64_t)4; ++ax0_ax1_fused_0_199) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_199 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_199 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3168))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_96 = 0; ax3_0_1_96 < (int64_t)2; ++ax3_0_1_96) {
    for (int64_t ax0_0_192 = 0; ax0_0_192 < (int64_t)4; ++ax0_0_192) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_192 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_96 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_192 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_192 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_192 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_192 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_193 = 0; ax0_0_193 < (int64_t)4; ++ax0_0_193) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_193 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_96 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_193 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_193 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_193 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_193 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_96 = 0; ax1_0_3_96 < 4; ++ax1_0_3_96) {
      for (int ax2_0_3_96 = 0; ax2_0_3_96 < 4; ++ax2_0_3_96) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_96) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_96) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_96) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_96) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_96) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_96) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_96) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_96) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_96) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_96) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_96) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_96) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_96) * (int64_t)32) + (((int64_t)ax2_0_3_96) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_200 = 0; ax0_ax1_fused_0_200 < (int64_t)4; ++ax0_ax1_fused_0_200) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_200 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_200 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3200))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_201 = 0; ax0_ax1_fused_0_201 < (int64_t)4; ++ax0_ax1_fused_0_201) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_201 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_201 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3200))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_97 = 0; ax3_0_1_97 < (int64_t)2; ++ax3_0_1_97) {
    for (int64_t ax0_0_194 = 0; ax0_0_194 < (int64_t)4; ++ax0_0_194) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_194 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_97 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_194 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_194 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_194 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_194 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_195 = 0; ax0_0_195 < (int64_t)4; ++ax0_0_195) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_195 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_97 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_195 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_195 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_195 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_195 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_97 = 0; ax1_0_3_97 < 4; ++ax1_0_3_97) {
      for (int ax2_0_3_97 = 0; ax2_0_3_97 < 4; ++ax2_0_3_97) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_97) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_97) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_97) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_97) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_97) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_97) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_97) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_97) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_97) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_97) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_97) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_97) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_97) * (int64_t)32) + (((int64_t)ax2_0_3_97) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_202 = 0; ax0_ax1_fused_0_202 < (int64_t)4; ++ax0_ax1_fused_0_202) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_202 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_202 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3232))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_203 = 0; ax0_ax1_fused_0_203 < (int64_t)4; ++ax0_ax1_fused_0_203) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_203 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_203 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3232))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_98 = 0; ax3_0_1_98 < (int64_t)2; ++ax3_0_1_98) {
    for (int64_t ax0_0_196 = 0; ax0_0_196 < (int64_t)4; ++ax0_0_196) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_196 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_98 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_196 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_196 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_196 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_196 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_197 = 0; ax0_0_197 < (int64_t)4; ++ax0_0_197) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_197 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_98 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_197 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_197 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_197 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_197 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_98 = 0; ax1_0_3_98 < 4; ++ax1_0_3_98) {
      for (int ax2_0_3_98 = 0; ax2_0_3_98 < 4; ++ax2_0_3_98) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_98) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_98) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_98) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_98) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_98) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_98) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_98) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_98) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_98) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_98) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_98) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_98) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_98) * (int64_t)32) + (((int64_t)ax2_0_3_98) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_204 = 0; ax0_ax1_fused_0_204 < (int64_t)4; ++ax0_ax1_fused_0_204) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_204 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_204 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3264))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_205 = 0; ax0_ax1_fused_0_205 < (int64_t)4; ++ax0_ax1_fused_0_205) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_205 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_205 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3264))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_99 = 0; ax3_0_1_99 < (int64_t)2; ++ax3_0_1_99) {
    for (int64_t ax0_0_198 = 0; ax0_0_198 < (int64_t)4; ++ax0_0_198) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_198 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_99 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_198 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_198 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_198 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_198 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_199 = 0; ax0_0_199 < (int64_t)4; ++ax0_0_199) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_199 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_99 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_199 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_199 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_199 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_199 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_99 = 0; ax1_0_3_99 < 4; ++ax1_0_3_99) {
      for (int ax2_0_3_99 = 0; ax2_0_3_99 < 4; ++ax2_0_3_99) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_99) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_99) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_99) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_99) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_99) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_99) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_99) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_99) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_99) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_99) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_99) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_99) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_99) * (int64_t)32) + (((int64_t)ax2_0_3_99) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_206 = 0; ax0_ax1_fused_0_206 < (int64_t)4; ++ax0_ax1_fused_0_206) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_206 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_206 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3296))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_207 = 0; ax0_ax1_fused_0_207 < (int64_t)4; ++ax0_ax1_fused_0_207) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_207 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_207 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3296))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_100 = 0; ax3_0_1_100 < (int64_t)2; ++ax3_0_1_100) {
    for (int64_t ax0_0_200 = 0; ax0_0_200 < (int64_t)4; ++ax0_0_200) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_200 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_100 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_200 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_200 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_200 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_200 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_201 = 0; ax0_0_201 < (int64_t)4; ++ax0_0_201) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_201 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_100 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_201 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_201 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_201 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_201 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_100 = 0; ax1_0_3_100 < 4; ++ax1_0_3_100) {
      for (int ax2_0_3_100 = 0; ax2_0_3_100 < 4; ++ax2_0_3_100) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_100) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_100) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_100) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_100) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_100) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_100) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_100) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_100) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_100) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_100) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_100) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_100) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_100) * (int64_t)32) + (((int64_t)ax2_0_3_100) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_208 = 0; ax0_ax1_fused_0_208 < (int64_t)4; ++ax0_ax1_fused_0_208) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_208 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_208 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3328))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_209 = 0; ax0_ax1_fused_0_209 < (int64_t)4; ++ax0_ax1_fused_0_209) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_209 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_209 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3328))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_101 = 0; ax3_0_1_101 < (int64_t)2; ++ax3_0_1_101) {
    for (int64_t ax0_0_202 = 0; ax0_0_202 < (int64_t)4; ++ax0_0_202) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_202 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_101 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_202 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_202 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_202 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_202 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_203 = 0; ax0_0_203 < (int64_t)4; ++ax0_0_203) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_203 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_101 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_203 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_203 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_203 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_203 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_101 = 0; ax1_0_3_101 < 4; ++ax1_0_3_101) {
      for (int ax2_0_3_101 = 0; ax2_0_3_101 < 4; ++ax2_0_3_101) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_101) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_101) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_101) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_101) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_101) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_101) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_101) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_101) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_101) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_101) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_101) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_101) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_101) * (int64_t)32) + (((int64_t)ax2_0_3_101) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_210 = 0; ax0_ax1_fused_0_210 < (int64_t)4; ++ax0_ax1_fused_0_210) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_210 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_210 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3360))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_211 = 0; ax0_ax1_fused_0_211 < (int64_t)4; ++ax0_ax1_fused_0_211) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_211 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_211 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3360))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_102 = 0; ax3_0_1_102 < (int64_t)2; ++ax3_0_1_102) {
    for (int64_t ax0_0_204 = 0; ax0_0_204 < (int64_t)4; ++ax0_0_204) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_204 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_102 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_204 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_204 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_204 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_204 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_205 = 0; ax0_0_205 < (int64_t)4; ++ax0_0_205) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_205 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_102 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_205 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_205 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_205 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_205 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_102 = 0; ax1_0_3_102 < 4; ++ax1_0_3_102) {
      for (int ax2_0_3_102 = 0; ax2_0_3_102 < 4; ++ax2_0_3_102) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_102) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_102) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_102) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_102) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_102) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_102) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_102) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_102) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_102) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_102) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_102) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_102) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_102) * (int64_t)32) + (((int64_t)ax2_0_3_102) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_212 = 0; ax0_ax1_fused_0_212 < (int64_t)4; ++ax0_ax1_fused_0_212) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_212 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_212 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3392))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_213 = 0; ax0_ax1_fused_0_213 < (int64_t)4; ++ax0_ax1_fused_0_213) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_213 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_213 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3392))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_103 = 0; ax3_0_1_103 < (int64_t)2; ++ax3_0_1_103) {
    for (int64_t ax0_0_206 = 0; ax0_0_206 < (int64_t)4; ++ax0_0_206) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_206 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_103 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_206 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_206 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_206 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_206 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_207 = 0; ax0_0_207 < (int64_t)4; ++ax0_0_207) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_207 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_103 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_207 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_207 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_207 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_207 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_103 = 0; ax1_0_3_103 < 4; ++ax1_0_3_103) {
      for (int ax2_0_3_103 = 0; ax2_0_3_103 < 4; ++ax2_0_3_103) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_103) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_103) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_103) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_103) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_103) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_103) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_103) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_103) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_103) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_103) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_103) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_103) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_103) * (int64_t)32) + (((int64_t)ax2_0_3_103) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_214 = 0; ax0_ax1_fused_0_214 < (int64_t)4; ++ax0_ax1_fused_0_214) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_214 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_214 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3424))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_215 = 0; ax0_ax1_fused_0_215 < (int64_t)4; ++ax0_ax1_fused_0_215) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_215 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_215 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3424))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_104 = 0; ax3_0_1_104 < (int64_t)2; ++ax3_0_1_104) {
    for (int64_t ax0_0_208 = 0; ax0_0_208 < (int64_t)4; ++ax0_0_208) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_208 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_104 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_208 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_208 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_208 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_208 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_209 = 0; ax0_0_209 < (int64_t)4; ++ax0_0_209) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_209 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_104 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_209 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_209 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_209 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_209 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_104 = 0; ax1_0_3_104 < 4; ++ax1_0_3_104) {
      for (int ax2_0_3_104 = 0; ax2_0_3_104 < 4; ++ax2_0_3_104) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_104) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_104) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_104) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_104) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_104) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_104) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_104) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_104) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_104) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_104) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_104) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_104) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_104) * (int64_t)32) + (((int64_t)ax2_0_3_104) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_216 = 0; ax0_ax1_fused_0_216 < (int64_t)4; ++ax0_ax1_fused_0_216) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_216 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_216 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3456))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_217 = 0; ax0_ax1_fused_0_217 < (int64_t)4; ++ax0_ax1_fused_0_217) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_217 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_217 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3456))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_105 = 0; ax3_0_1_105 < (int64_t)2; ++ax3_0_1_105) {
    for (int64_t ax0_0_210 = 0; ax0_0_210 < (int64_t)4; ++ax0_0_210) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_210 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_105 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_210 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_210 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_210 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_210 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_211 = 0; ax0_0_211 < (int64_t)4; ++ax0_0_211) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_211 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_105 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_211 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_211 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_211 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_211 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_105 = 0; ax1_0_3_105 < 4; ++ax1_0_3_105) {
      for (int ax2_0_3_105 = 0; ax2_0_3_105 < 4; ++ax2_0_3_105) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_105) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_105) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_105) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_105) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_105) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_105) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_105) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_105) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_105) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_105) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_105) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_105) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_105) * (int64_t)32) + (((int64_t)ax2_0_3_105) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_218 = 0; ax0_ax1_fused_0_218 < (int64_t)4; ++ax0_ax1_fused_0_218) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_218 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_218 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3488))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_219 = 0; ax0_ax1_fused_0_219 < (int64_t)4; ++ax0_ax1_fused_0_219) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_219 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_219 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3488))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_106 = 0; ax3_0_1_106 < (int64_t)2; ++ax3_0_1_106) {
    for (int64_t ax0_0_212 = 0; ax0_0_212 < (int64_t)4; ++ax0_0_212) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_212 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_106 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_212 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_212 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_212 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_212 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_213 = 0; ax0_0_213 < (int64_t)4; ++ax0_0_213) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_213 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_106 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_213 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_213 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_213 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_213 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_106 = 0; ax1_0_3_106 < 4; ++ax1_0_3_106) {
      for (int ax2_0_3_106 = 0; ax2_0_3_106 < 4; ++ax2_0_3_106) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_106) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_106) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_106) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_106) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_106) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_106) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_106) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_106) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_106) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_106) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_106) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_106) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_106) * (int64_t)32) + (((int64_t)ax2_0_3_106) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_220 = 0; ax0_ax1_fused_0_220 < (int64_t)4; ++ax0_ax1_fused_0_220) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_220 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_220 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3520))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_221 = 0; ax0_ax1_fused_0_221 < (int64_t)4; ++ax0_ax1_fused_0_221) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_221 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_221 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3520))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_107 = 0; ax3_0_1_107 < (int64_t)2; ++ax3_0_1_107) {
    for (int64_t ax0_0_214 = 0; ax0_0_214 < (int64_t)4; ++ax0_0_214) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_214 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_107 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_214 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_214 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_214 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_214 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_215 = 0; ax0_0_215 < (int64_t)4; ++ax0_0_215) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_215 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_107 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_215 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_215 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_215 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_215 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_107 = 0; ax1_0_3_107 < 4; ++ax1_0_3_107) {
      for (int ax2_0_3_107 = 0; ax2_0_3_107 < 4; ++ax2_0_3_107) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_107) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_107) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_107) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_107) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_107) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_107) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_107) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_107) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_107) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_107) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_107) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_107) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_107) * (int64_t)32) + (((int64_t)ax2_0_3_107) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_222 = 0; ax0_ax1_fused_0_222 < (int64_t)4; ++ax0_ax1_fused_0_222) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_222 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_222 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3552))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_223 = 0; ax0_ax1_fused_0_223 < (int64_t)4; ++ax0_ax1_fused_0_223) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_223 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_223 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3552))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_108 = 0; ax3_0_1_108 < (int64_t)2; ++ax3_0_1_108) {
    for (int64_t ax0_0_216 = 0; ax0_0_216 < (int64_t)4; ++ax0_0_216) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_216 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_108 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_216 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_216 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_216 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_216 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_217 = 0; ax0_0_217 < (int64_t)4; ++ax0_0_217) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_217 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_108 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_217 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_217 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_217 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_217 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_108 = 0; ax1_0_3_108 < 4; ++ax1_0_3_108) {
      for (int ax2_0_3_108 = 0; ax2_0_3_108 < 4; ++ax2_0_3_108) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_108) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_108) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_108) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_108) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_108) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_108) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_108) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_108) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_108) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_108) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_108) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_108) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_108) * (int64_t)32) + (((int64_t)ax2_0_3_108) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_224 = 0; ax0_ax1_fused_0_224 < (int64_t)4; ++ax0_ax1_fused_0_224) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_224 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_224 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3584))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_225 = 0; ax0_ax1_fused_0_225 < (int64_t)4; ++ax0_ax1_fused_0_225) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_225 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_225 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3584))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_109 = 0; ax3_0_1_109 < (int64_t)2; ++ax3_0_1_109) {
    for (int64_t ax0_0_218 = 0; ax0_0_218 < (int64_t)4; ++ax0_0_218) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_218 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_109 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_218 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_218 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_218 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_218 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_219 = 0; ax0_0_219 < (int64_t)4; ++ax0_0_219) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_219 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_109 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_219 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_219 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_219 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_219 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_109 = 0; ax1_0_3_109 < 4; ++ax1_0_3_109) {
      for (int ax2_0_3_109 = 0; ax2_0_3_109 < 4; ++ax2_0_3_109) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_109) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_109) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_109) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_109) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_109) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_109) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_109) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_109) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_109) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_109) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_109) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_109) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_109) * (int64_t)32) + (((int64_t)ax2_0_3_109) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_226 = 0; ax0_ax1_fused_0_226 < (int64_t)4; ++ax0_ax1_fused_0_226) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_226 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_226 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3616))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_227 = 0; ax0_ax1_fused_0_227 < (int64_t)4; ++ax0_ax1_fused_0_227) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_227 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_227 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3616))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_110 = 0; ax3_0_1_110 < (int64_t)2; ++ax3_0_1_110) {
    for (int64_t ax0_0_220 = 0; ax0_0_220 < (int64_t)4; ++ax0_0_220) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_220 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_110 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_220 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_220 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_220 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_220 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_221 = 0; ax0_0_221 < (int64_t)4; ++ax0_0_221) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_221 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_110 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_221 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_221 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_221 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_221 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_110 = 0; ax1_0_3_110 < 4; ++ax1_0_3_110) {
      for (int ax2_0_3_110 = 0; ax2_0_3_110 < 4; ++ax2_0_3_110) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_110) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_110) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_110) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_110) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_110) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_110) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_110) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_110) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_110) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_110) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_110) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_110) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_110) * (int64_t)32) + (((int64_t)ax2_0_3_110) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_228 = 0; ax0_ax1_fused_0_228 < (int64_t)4; ++ax0_ax1_fused_0_228) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_228 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_228 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3648))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_229 = 0; ax0_ax1_fused_0_229 < (int64_t)4; ++ax0_ax1_fused_0_229) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_229 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_229 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3648))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_111 = 0; ax3_0_1_111 < (int64_t)2; ++ax3_0_1_111) {
    for (int64_t ax0_0_222 = 0; ax0_0_222 < (int64_t)4; ++ax0_0_222) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_222 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_111 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_222 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_222 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_222 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_222 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_223 = 0; ax0_0_223 < (int64_t)4; ++ax0_0_223) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_223 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_111 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_223 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_223 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_223 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_223 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_111 = 0; ax1_0_3_111 < 4; ++ax1_0_3_111) {
      for (int ax2_0_3_111 = 0; ax2_0_3_111 < 4; ++ax2_0_3_111) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_111) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_111) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_111) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_111) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_111) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_111) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_111) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_111) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_111) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_111) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_111) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_111) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_111) * (int64_t)32) + (((int64_t)ax2_0_3_111) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_230 = 0; ax0_ax1_fused_0_230 < (int64_t)4; ++ax0_ax1_fused_0_230) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_230 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_230 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3680))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_231 = 0; ax0_ax1_fused_0_231 < (int64_t)4; ++ax0_ax1_fused_0_231) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_231 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_231 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3680))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_112 = 0; ax3_0_1_112 < (int64_t)2; ++ax3_0_1_112) {
    for (int64_t ax0_0_224 = 0; ax0_0_224 < (int64_t)4; ++ax0_0_224) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_224 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_112 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_224 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_224 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_224 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_224 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_225 = 0; ax0_0_225 < (int64_t)4; ++ax0_0_225) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_225 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_112 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_225 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_225 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_225 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_225 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_112 = 0; ax1_0_3_112 < 4; ++ax1_0_3_112) {
      for (int ax2_0_3_112 = 0; ax2_0_3_112 < 4; ++ax2_0_3_112) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_112) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_112) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_112) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_112) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_112) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_112) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_112) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_112) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_112) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_112) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_112) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_112) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_112) * (int64_t)32) + (((int64_t)ax2_0_3_112) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_232 = 0; ax0_ax1_fused_0_232 < (int64_t)4; ++ax0_ax1_fused_0_232) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_232 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_232 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3712))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_233 = 0; ax0_ax1_fused_0_233 < (int64_t)4; ++ax0_ax1_fused_0_233) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_233 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_233 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3712))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_113 = 0; ax3_0_1_113 < (int64_t)2; ++ax3_0_1_113) {
    for (int64_t ax0_0_226 = 0; ax0_0_226 < (int64_t)4; ++ax0_0_226) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_226 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_113 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_226 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_226 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_226 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_226 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_227 = 0; ax0_0_227 < (int64_t)4; ++ax0_0_227) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_227 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_113 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_227 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_227 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_227 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_227 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_113 = 0; ax1_0_3_113 < 4; ++ax1_0_3_113) {
      for (int ax2_0_3_113 = 0; ax2_0_3_113 < 4; ++ax2_0_3_113) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_113) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_113) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_113) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_113) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_113) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_113) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_113) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_113) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_113) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_113) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_113) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_113) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_113) * (int64_t)32) + (((int64_t)ax2_0_3_113) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_234 = 0; ax0_ax1_fused_0_234 < (int64_t)4; ++ax0_ax1_fused_0_234) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_234 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_234 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3744))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_235 = 0; ax0_ax1_fused_0_235 < (int64_t)4; ++ax0_ax1_fused_0_235) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_235 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_235 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3744))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_114 = 0; ax3_0_1_114 < (int64_t)2; ++ax3_0_1_114) {
    for (int64_t ax0_0_228 = 0; ax0_0_228 < (int64_t)4; ++ax0_0_228) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_228 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_114 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_228 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_228 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_228 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_228 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_229 = 0; ax0_0_229 < (int64_t)4; ++ax0_0_229) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_229 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_114 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_229 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_229 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_229 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_229 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_114 = 0; ax1_0_3_114 < 4; ++ax1_0_3_114) {
      for (int ax2_0_3_114 = 0; ax2_0_3_114 < 4; ++ax2_0_3_114) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_114) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_114) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_114) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_114) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_114) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_114) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_114) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_114) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_114) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_114) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_114) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_114) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_114) * (int64_t)32) + (((int64_t)ax2_0_3_114) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_236 = 0; ax0_ax1_fused_0_236 < (int64_t)4; ++ax0_ax1_fused_0_236) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_236 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_236 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3776))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_237 = 0; ax0_ax1_fused_0_237 < (int64_t)4; ++ax0_ax1_fused_0_237) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_237 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_237 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3776))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_115 = 0; ax3_0_1_115 < (int64_t)2; ++ax3_0_1_115) {
    for (int64_t ax0_0_230 = 0; ax0_0_230 < (int64_t)4; ++ax0_0_230) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_230 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_115 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_230 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_230 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_230 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_230 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_231 = 0; ax0_0_231 < (int64_t)4; ++ax0_0_231) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_231 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_115 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_231 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_231 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_231 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_231 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_115 = 0; ax1_0_3_115 < 4; ++ax1_0_3_115) {
      for (int ax2_0_3_115 = 0; ax2_0_3_115 < 4; ++ax2_0_3_115) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_115) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_115) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_115) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_115) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_115) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_115) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_115) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_115) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_115) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_115) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_115) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_115) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_115) * (int64_t)32) + (((int64_t)ax2_0_3_115) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_238 = 0; ax0_ax1_fused_0_238 < (int64_t)4; ++ax0_ax1_fused_0_238) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_238 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_238 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3808))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_239 = 0; ax0_ax1_fused_0_239 < (int64_t)4; ++ax0_ax1_fused_0_239) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_239 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_239 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3808))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_116 = 0; ax3_0_1_116 < (int64_t)2; ++ax3_0_1_116) {
    for (int64_t ax0_0_232 = 0; ax0_0_232 < (int64_t)4; ++ax0_0_232) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_232 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_116 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_232 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_232 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_232 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_232 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_233 = 0; ax0_0_233 < (int64_t)4; ++ax0_0_233) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_233 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_116 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_233 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_233 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_233 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_233 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_116 = 0; ax1_0_3_116 < 4; ++ax1_0_3_116) {
      for (int ax2_0_3_116 = 0; ax2_0_3_116 < 4; ++ax2_0_3_116) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_116) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_116) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_116) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_116) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_116) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_116) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_116) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_116) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_116) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_116) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_116) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_116) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_116) * (int64_t)32) + (((int64_t)ax2_0_3_116) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_240 = 0; ax0_ax1_fused_0_240 < (int64_t)4; ++ax0_ax1_fused_0_240) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_240 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_240 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3840))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_241 = 0; ax0_ax1_fused_0_241 < (int64_t)4; ++ax0_ax1_fused_0_241) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_241 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_241 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3840))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_117 = 0; ax3_0_1_117 < (int64_t)2; ++ax3_0_1_117) {
    for (int64_t ax0_0_234 = 0; ax0_0_234 < (int64_t)4; ++ax0_0_234) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_234 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_117 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_234 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_234 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_234 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_234 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_235 = 0; ax0_0_235 < (int64_t)4; ++ax0_0_235) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_235 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_117 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_235 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_235 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_235 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_235 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_117 = 0; ax1_0_3_117 < 4; ++ax1_0_3_117) {
      for (int ax2_0_3_117 = 0; ax2_0_3_117 < 4; ++ax2_0_3_117) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_117) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_117) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_117) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_117) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_117) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_117) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_117) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_117) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_117) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_117) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_117) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_117) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_117) * (int64_t)32) + (((int64_t)ax2_0_3_117) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_242 = 0; ax0_ax1_fused_0_242 < (int64_t)4; ++ax0_ax1_fused_0_242) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_242 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_242 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3872))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_243 = 0; ax0_ax1_fused_0_243 < (int64_t)4; ++ax0_ax1_fused_0_243) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_243 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_243 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3872))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_118 = 0; ax3_0_1_118 < (int64_t)2; ++ax3_0_1_118) {
    for (int64_t ax0_0_236 = 0; ax0_0_236 < (int64_t)4; ++ax0_0_236) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_236 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_118 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_236 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_236 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_236 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_236 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_237 = 0; ax0_0_237 < (int64_t)4; ++ax0_0_237) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_237 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_118 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_237 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_237 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_237 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_237 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_118 = 0; ax1_0_3_118 < 4; ++ax1_0_3_118) {
      for (int ax2_0_3_118 = 0; ax2_0_3_118 < 4; ++ax2_0_3_118) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_118) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_118) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_118) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_118) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_118) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_118) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_118) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_118) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_118) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_118) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_118) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_118) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_118) * (int64_t)32) + (((int64_t)ax2_0_3_118) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_244 = 0; ax0_ax1_fused_0_244 < (int64_t)4; ++ax0_ax1_fused_0_244) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_244 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_244 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3904))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_245 = 0; ax0_ax1_fused_0_245 < (int64_t)4; ++ax0_ax1_fused_0_245) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_245 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_245 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3904))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_119 = 0; ax3_0_1_119 < (int64_t)2; ++ax3_0_1_119) {
    for (int64_t ax0_0_238 = 0; ax0_0_238 < (int64_t)4; ++ax0_0_238) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_238 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_119 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_238 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_238 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_238 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_238 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_239 = 0; ax0_0_239 < (int64_t)4; ++ax0_0_239) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_239 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_119 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_239 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_239 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_239 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_239 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_119 = 0; ax1_0_3_119 < 4; ++ax1_0_3_119) {
      for (int ax2_0_3_119 = 0; ax2_0_3_119 < 4; ++ax2_0_3_119) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_119) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_119) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_119) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_119) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_119) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_119) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_119) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_119) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_119) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_119) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_119) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_119) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_119) * (int64_t)32) + (((int64_t)ax2_0_3_119) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_246 = 0; ax0_ax1_fused_0_246 < (int64_t)4; ++ax0_ax1_fused_0_246) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_246 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_246 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3936))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_247 = 0; ax0_ax1_fused_0_247 < (int64_t)4; ++ax0_ax1_fused_0_247) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_247 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_247 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3936))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_120 = 0; ax3_0_1_120 < (int64_t)2; ++ax3_0_1_120) {
    for (int64_t ax0_0_240 = 0; ax0_0_240 < (int64_t)4; ++ax0_0_240) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_240 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_120 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_240 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_240 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_240 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_240 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_241 = 0; ax0_0_241 < (int64_t)4; ++ax0_0_241) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_241 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_120 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_241 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_241 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_241 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_241 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_120 = 0; ax1_0_3_120 < 4; ++ax1_0_3_120) {
      for (int ax2_0_3_120 = 0; ax2_0_3_120 < 4; ++ax2_0_3_120) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_120) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_120) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_120) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_120) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_120) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_120) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_120) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_120) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_120) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_120) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_120) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_120) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_120) * (int64_t)32) + (((int64_t)ax2_0_3_120) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_248 = 0; ax0_ax1_fused_0_248 < (int64_t)4; ++ax0_ax1_fused_0_248) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_248 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)32768));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_248 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3968))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_249 = 0; ax0_ax1_fused_0_249 < (int64_t)4; ++ax0_ax1_fused_0_249) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + (((((ax0_ax1_fused_0_249 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_249 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)3968))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_121 = 0; ax3_0_1_121 < (int64_t)2; ++ax3_0_1_121) {
    for (int64_t ax0_0_242 = 0; ax0_0_242 < (int64_t)4; ++ax0_0_242) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_242 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_121 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_242 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_242 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_242 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_242 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_243 = 0; ax0_0_243 < (int64_t)4; ++ax0_0_243) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_243 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_121 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_243 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_243 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_243 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_243 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_121 = 0; ax1_0_3_121 < 4; ++ax1_0_3_121) {
      for (int ax2_0_3_121 = 0; ax2_0_3_121 < 4; ++ax2_0_3_121) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_121) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_121) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_121) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_121) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_121) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_121) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_121) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_121) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_121) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_121) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_121) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_121) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_121) * (int64_t)32) + (((int64_t)ax2_0_3_121) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_250 = 0; ax0_ax1_fused_0_250 < (int64_t)4; ++ax0_ax1_fused_0_250) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_250 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)40960));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_250 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)4000))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_251 = 0; ax0_ax1_fused_0_251 < (int64_t)4; ++ax0_ax1_fused_0_251) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_251 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)8192));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_251 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)4000))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_122 = 0; ax3_0_1_122 < (int64_t)2; ++ax3_0_1_122) {
    for (int64_t ax0_0_244 = 0; ax0_0_244 < (int64_t)4; ++ax0_0_244) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_244 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_122 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_244 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_244 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_244 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_244 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_245 = 0; ax0_0_245 < (int64_t)4; ++ax0_0_245) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_245 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_122 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_245 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_245 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_245 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_245 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_122 = 0; ax1_0_3_122 < 4; ++ax1_0_3_122) {
      for (int ax2_0_3_122 = 0; ax2_0_3_122 < 4; ++ax2_0_3_122) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_122) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_122) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_122) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_122) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_122) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_122) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_122) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_122) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_122) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_122) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_122) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_122) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_122) * (int64_t)32) + (((int64_t)ax2_0_3_122) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_252 = 0; ax0_ax1_fused_0_252 < (int64_t)4; ++ax0_ax1_fused_0_252) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_252 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)49152));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_252 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)4032))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_253 = 0; ax0_ax1_fused_0_253 < (int64_t)4; ++ax0_ax1_fused_0_253) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_253 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)16384));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_253 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)4032))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_123 = 0; ax3_0_1_123 < (int64_t)2; ++ax3_0_1_123) {
    for (int64_t ax0_0_246 = 0; ax0_0_246 < (int64_t)4; ++ax0_0_246) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_246 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_123 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_246 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_246 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_246 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_246 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_247 = 0; ax0_0_247 < (int64_t)4; ++ax0_0_247) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_247 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_123 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_247 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_247 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_247 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_247 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_123 = 0; ax1_0_3_123 < 4; ++ax1_0_3_123) {
      for (int ax2_0_3_123 = 0; ax2_0_3_123 < 4; ++ax2_0_3_123) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_123) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_123) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_123) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_123) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_123) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_123) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_123) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_123) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_123) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_123) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_123) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_123) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_123) * (int64_t)32) + (((int64_t)ax2_0_3_123) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_254 = 0; ax0_ax1_fused_0_254 < (int64_t)4; ++ax0_ax1_fused_0_254) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_254 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)57344));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_254 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)4064))), "n"(16)
    );
  }
  }
  for (int64_t ax0_ax1_fused_0_255 = 0; ax0_ax1_fused_0_255 < (int64_t)4; ++ax0_ax1_fused_0_255) {

  {
    unsigned int addr = cast_smem_ptr_to_int(buf_dyn_shmem + ((((((ax0_ax1_fused_0_255 * (int64_t)2048) + (((int64_t)threadIdx.z) * (int64_t)1024)) + (((int64_t)threadIdx.y) * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)64)) + (((((int64_t)threadIdx.x) & (int64_t)3) ^ (((int64_t)threadIdx.x) >> (int64_t)3)) * (int64_t)16)) + (int64_t)24576));
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)524288) + (ax0_ax1_fused_0_255 * (int64_t)131072)) + (((int64_t)threadIdx.z) * (int64_t)65536)) + (((int64_t)threadIdx.y) * (int64_t)32768)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)4096)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)8)) + (int64_t)4064))), "n"(16)
    );
  }
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 3;");

  __syncthreads();
  for (int64_t ax3_0_1_124 = 0; ax3_0_1_124 < (int64_t)2; ++ax3_0_1_124) {
    for (int64_t ax0_0_248 = 0; ax0_0_248 < (int64_t)4; ++ax0_0_248) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_248 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_124 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)16384)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_248 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_248 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_248 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp + (ax0_0_248 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_249 = 0; ax0_0_249 < (int64_t)4; ++ax0_0_249) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_249 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_124 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8))])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_249 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_249 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_249 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (ax0_0_249 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_124 = 0; ax1_0_3_124 < 4; ++ax1_0_3_124) {
      for (int ax2_0_3_124 = 0; ax2_0_3_124 < 4; ++ax2_0_3_124) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_124) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_124) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_124) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_124) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_124) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + (((int64_t)ax2_0_3_124) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_124) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_124) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_124) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp + (((int64_t)ax1_0_3_124) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_124) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp + ((((int64_t)ax2_0_3_124) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_124) * (int64_t)32) + (((int64_t)ax2_0_3_124) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
__asm__ __volatile__("cp.async.wait_group 2;");

  __syncthreads();
  for (int64_t ax3_0_1_125 = 0; ax3_0_1_125 < (int64_t)2; ++ax3_0_1_125) {
    for (int64_t ax0_0_250 = 0; ax0_0_250 < (int64_t)4; ++ax0_0_250) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_250 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_125 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)20480)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (ax0_0_250 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (ax0_0_250 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (ax0_0_250 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (ax0_0_250 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_251 = 0; ax0_0_251 < (int64_t)4; ++ax0_0_251) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_251 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_125 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)4096)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (ax0_0_251 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (ax0_0_251 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (ax0_0_251 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (ax0_0_251 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_125 = 0; ax1_0_3_125 < 4; ++ax1_0_3_125) {
      for (int ax2_0_3_125 = 0; ax2_0_3_125 < 4; ++ax2_0_3_125) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_125) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_125) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_125) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_125) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (((int64_t)ax2_0_3_125) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (((int64_t)ax2_0_3_125) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_125) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_125) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_125) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_125) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + ((((int64_t)ax2_0_3_125) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + ((((int64_t)ax2_0_3_125) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_125) * (int64_t)32) + (((int64_t)ax2_0_3_125) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
__asm__ __volatile__("cp.async.wait_group 1;");

  __syncthreads();
  for (int64_t ax3_0_1_126 = 0; ax3_0_1_126 < (int64_t)2; ++ax3_0_1_126) {
    for (int64_t ax0_0_252 = 0; ax0_0_252 < (int64_t)4; ++ax0_0_252) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_252 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_126 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)24576)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (ax0_0_252 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (ax0_0_252 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (ax0_0_252 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (ax0_0_252 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_253 = 0; ax0_0_253 < (int64_t)4; ++ax0_0_253) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_253 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_126 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)8192)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (ax0_0_253 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (ax0_0_253 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (ax0_0_253 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (ax0_0_253 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_126 = 0; ax1_0_3_126 < 4; ++ax1_0_3_126) {
      for (int ax2_0_3_126 = 0; ax2_0_3_126 < 4; ++ax2_0_3_126) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_126) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_126) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_126) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_126) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (((int64_t)ax2_0_3_126) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (((int64_t)ax2_0_3_126) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_126) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_126) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_126) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_126) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + ((((int64_t)ax2_0_3_126) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + ((((int64_t)ax2_0_3_126) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_126) * (int64_t)32) + (((int64_t)ax2_0_3_126) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
__asm__ __volatile__("cp.async.wait_group 0;");

  __syncthreads();
  for (int64_t ax3_0_1_127 = 0; ax3_0_1_127 < (int64_t)2; ++ax3_0_1_127) {
    for (int64_t ax0_0_254 = 0; ax0_0_254 < (int64_t)4; ++ax0_0_254) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[(((((((int64_t)threadIdx.z) * (int64_t)2048) + (ax0_0_254 * (int64_t)512)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)32)) + ((((ax3_0_1_127 * (int64_t)2) + (((int64_t)threadIdx.x) >> (int64_t)4)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)28672)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (ax0_0_254 * (int64_t)8)))[0]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (ax0_0_254 * (int64_t)8)))[1]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (ax0_0_254 * (int64_t)8)))[2]), "=r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (ax0_0_254 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int64_t ax0_0_255 = 0; ax0_0_255 < (int64_t)4; ++ax0_0_255) {

  {
    unsigned int addr = cast_smem_ptr_to_int((&(((half*)buf_dyn_shmem)[((((((((int64_t)threadIdx.y) * (int64_t)2048) + (ax0_0_255 * (int64_t)512)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)256)) + ((((int64_t)threadIdx.x) & (int64_t)7) * (int64_t)32)) + ((((ax3_0_1_127 * (int64_t)2) + ((((int64_t)threadIdx.x) & (int64_t)15) >> (int64_t)3)) ^ ((((int64_t)threadIdx.x) & (int64_t)7) >> (int64_t)1)) * (int64_t)8)) + (int64_t)12288)])) + (int64_t)0);
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (ax0_0_255 * (int64_t)8)))[0]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (ax0_0_255 * (int64_t)8)))[1]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (ax0_0_255 * (int64_t)8)))[2]), "=r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (ax0_0_255 * (int64_t)8)))[3])
      : "r"(addr)
    );
  }
    }
    for (int ax1_0_3_127 = 0; ax1_0_3_127 < 4; ++ax1_0_3_127) {
      for (int ax2_0_3_127 = 0; ax2_0_3_127 < 4; ++ax2_0_3_127) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8))))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8))))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8))))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_127) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_127) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_127) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_127) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (((int64_t)ax2_0_3_127) * (int64_t)8)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + (((int64_t)ax2_0_3_127) * (int64_t)8)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8))))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8))))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8))))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + ((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8)) + (int64_t)4)))[0]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8)) + (int64_t)4)))[1]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8)) + (int64_t)4)))[2]), "=f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8)) + (int64_t)4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_127) * (int64_t)8)))[0]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_127) * (int64_t)8)))[1]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_127) * (int64_t)8)))[2]), "r"(((unsigned *)(A_reindex_shared_dyn_warp_1 + (((int64_t)ax1_0_3_127) * (int64_t)8)))[3]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + ((((int64_t)ax2_0_3_127) * (int64_t)8) + (int64_t)4)))[0]), "r"(((unsigned *)(T_transpose_reindex_shared_dyn_warp_1 + ((((int64_t)ax2_0_3_127) * (int64_t)8) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8)) + (int64_t)4)))[0]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8)) + (int64_t)4)))[1]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8)) + (int64_t)4)))[2]), "f"(((float *)(matmul_reindex_shared_dyn_warp + (((((int64_t)ax1_0_3_127) * (int64_t)32) + (((int64_t)ax2_0_3_127) * (int64_t)8)) + (int64_t)4)))[3]));
  }
      }
    }
  }
  __syncthreads();
  for (int64_t ax1_1 = 0; ax1_1 < (int64_t)4; ++ax1_1) {
    for (int64_t ax2_1 = 0; ax2_1 < (int64_t)4; ++ax2_1) {
      for (int64_t local_id = 0; local_id < (int64_t)8; ++local_id) {
        ((float*)buf_dyn_shmem)[(((((((((int64_t)threadIdx.z) * (int64_t)8192) + (ax1_1 * (int64_t)2048)) + (((local_id & (int64_t)3) >> (int64_t)1) * (int64_t)1024)) + ((((int64_t)threadIdx.x) >> (int64_t)2) * (int64_t)128)) + (((((((int64_t)threadIdx.y) * (int64_t)8) + (ax2_1 * (int64_t)2)) + (local_id >> (int64_t)2)) ^ (((int64_t)threadIdx.x) >> (int64_t)2)) * (int64_t)8)) + ((((int64_t)threadIdx.x) & (int64_t)3) * (int64_t)2)) + (local_id & (int64_t)1))] = matmul_reindex_shared_dyn_warp[(((ax1_1 * (int64_t)32) + (ax2_1 * (int64_t)8)) + local_id)];
      }
    }
  }
  __syncthreads();
  for (int64_t ax0_ax1_fused_0_256 = 0; ax0_ax1_fused_0_256 < (int64_t)16; ++ax0_ax1_fused_0_256) {
    uint4 __1;
    ulonglong4 v_ = *(ulonglong4*)(((float*)buf_dyn_shmem) + (((((ax0_ax1_fused_0_256 * (int64_t)1024) + (((int64_t)threadIdx.z) * (int64_t)512)) + (((int64_t)threadIdx.y) * (int64_t)256)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)128)) + (((((int64_t)threadIdx.x) & (int64_t)15) ^ (((((int64_t)threadIdx.z) * (int64_t)4) + (((int64_t)threadIdx.y) * (int64_t)2)) + (((int64_t)threadIdx.x) >> (int64_t)4))) * (int64_t)8)));
    ((half2*)(&(__1.x)))->x = (half)(((float2*)(&(v_.x)))->x);
    ((half2*)(&(__1.x)))->y = (half)(((float2*)(&(v_.x)))->y);
    ((half2*)(&(__1.y)))->x = (half)(((float2*)(&(v_.y)))->x);
    ((half2*)(&(__1.y)))->y = (half)(((float2*)(&(v_.y)))->y);
    ((half2*)(&(__1.z)))->x = (half)(((float2*)(&(v_.z)))->x);
    ((half2*)(&(__1.z)))->y = (half)(((float2*)(&(v_.z)))->y);
    ((half2*)(&(__1.w)))->x = (half)(((float2*)(&(v_.w)))->x);
    ((half2*)(&(__1.w)))->y = (half)(((float2*)(&(v_.w)))->y);
    *(uint4*)(compute + ((((((((((int64_t)((int)blockIdx.x)) >> (int64_t)5) * (int64_t)524288) + (ax0_ax1_fused_0_256 * (int64_t)32768)) + (((int64_t)threadIdx.z) * (int64_t)16384)) + (((int64_t)threadIdx.y) * (int64_t)8192)) + ((((int64_t)threadIdx.x) >> (int64_t)4) * (int64_t)4096)) + ((((int64_t)((int)blockIdx.x)) & (int64_t)31) * (int64_t)128)) + ((((int64_t)threadIdx.x) & (int64_t)15) * (int64_t)8))) = __1;
  }
}


